{"cells":[{"metadata":{"id":"40A8377F0F6D4473942199BB33282EFB","mdEditEnable":false},"cell_type":"markdown","source":"# **处理最终测试集，因为测试集每行最后都多了一个\"\\t\"**"},{"metadata":{"id":"4E34C58FC63E427F877A8D30AECDCA27","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import os\ndata_path=\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\"\ndata_test=\"/home/kesci/test2.csv\"\nfw=open(data_test,\"w\")\nall_data=open(data_path)\nfor line in all_data:\n    fw.write(line.strip()+\"\\n\")\nfw.close()","execution_count":1},{"metadata":{"id":"6E61D3A5717E45468A3E92E313A32C83","mdEditEnable":false},"cell_type":"markdown","source":"# 生成128维word2vec词向量以及128维FastText词向量，并生成word2id文件"},{"metadata":{"id":"DD20DB016A6341FD93EDCB5E06B69657","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 生成word2vec词向量 128维\r\nimport gensim\r\nfrom gensim.models import Word2Vec\r\nfrom gensim.test.utils import get_tmpfile\r\nimport multiprocessing\r\nfrom random import random\r\n\r\npath = get_tmpfile(\"/home/kesci/word2vec.model\")  # 创建临时文件\r\n\r\nsentences=[] #句子的list\r\nf = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nfor line in f: \r\n    line = line.strip().split(\",\")\r\n    if random()<0.1:  #以0.1的概率随机抽取句子title和query\r\n        sentences.append(line[1])\r\n        sentences.append(line[3])\r\nf.close()\r\n\r\nf=open(\"/home/kesci/input/bytedance/test_final_part1.csv\")\r\nfor line in f:\r\n    line = line.strip().split(\",\")\r\n    sentences.append(line[1])\r\n    sentences.append(line[3])\r\nf.close()\r\n\r\n\r\nsentences=[]\r\nf = open(\"/home/kesci/test2.csv\")\r\nfor line in f:\r\n    line = line.strip().split(\",\")\r\n    sentences.append(line[1])\r\n    sentences.append(line[3])\r\nf.close()\r\nprint(len(sentences))\r\n\r\nclass MySentences(object):\r\n    def __init__(self, sentences):\r\n        self.sentences = sentences\r\n\r\n    def __iter__(self):\r\n        print(\"djkbakjs\")\r\n        for line in self.sentences:\r\n            yield line.split()\r\n        \r\nall_sentences = MySentences(sentences)\r\nmodel = Word2Vec(all_sentences, size=128, window=5, min_count=5, workers=4)\r\nmodel.save(\"/home/kesci/word2vec.model\")\r\n\r\n# 生成FastText词向量 128维\r\nfrom gensim.models import FastText\r\npath = get_tmpfile(\"./data/fast_w2v.model\")  # 创建临时文件\r\n\r\n\r\nclass MySentences(object):\r\n    def __init__(self, sentences):\r\n        self.sentences = sentences\r\n\r\n    def __iter__(self):\r\n        print(\"djkbakjs\")\r\n        for line in self.sentences:\r\n            yield line.split()\r\n\r\n\r\nall_sentences = MySentences(sentences)\r\nmodel = FastText(all_sentences, size=128, window=5, min_count=5, workers=4)\r\nmodel.save(\"./data/fast_w2v.model\")\r\n\r\n# 生成word2id文件，统计每个词的词频，从大到小排序之后，并且去除词频小于5的词\r\nmy_dict={}\r\nfor ju in sentences:\r\n    for w in ju.split():\r\n        my_dict.setdefault(w,0)\r\n        my_dict[w]+=1\r\np_d=sorted(my_dict.items(),key=lambda item:item[1],reverse=True)\r\nf_w=open(\"./data/word2id.txt\",\"w\")\r\nidx=1\r\nsum_word=0\r\nfor i in p_d:\r\n    if i[1]>=5:\r\n        sum_word+=1\r\n        f_w.write(i[0]+\" \"+str(idx)+\"\\n\")\r\n        idx += 1\r\nprint(sum_word)\r\nf_w.close()","execution_count":null},{"metadata":{"id":"9131FDD33A824AEC835DE621FC548475","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"# **分割数据，将10亿数据按顺序平均分成20份**"},{"metadata":{"id":"E2E35DCFF3C94DED9B411E0E8CF38137","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import os\ndata_path=\"/home/kesci/input/bytedance/train_final.csv\"\nsplit_data=\"/home/kesci/order_data\"\nlength=1000000000 \n\nnum_kuai=20\nkuai_size=length//num_kuai\n\nall_data=open(data_path)\nfw=open(os.path.join(split_data,\"0.csv\"),\"w\")\nkuai_num=0\nfor i,line in enumerate(all_data,1):\n    fw.write(line.strip()+\"\\n\") # 因为训练集每行最后也有\\t，所以需要用line.strip()去除\\t\n    if i%kuai_size==0:\n        kuai_num+=1\n        fw.close()\n        if kuai_num==20:\n            break\n        fw = open(os.path.join(split_data,str(kuai_num)+\".csv\"),\"w\")\nfw.close()","execution_count":null},{"metadata":{"id":"BB3B1076F295425A88B8D78265FEDB09","mdEditEnable":false},"cell_type":"markdown","source":"# 分割最后5000万数据，生成本地的验证集，就是最后的5万数据。"},{"metadata":{"id":"756A267213ED41EA8B21247DE20C71B2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"f=open(\"/home/kesci/order_data/19.csv\")\nfw=open(\"/home/kesci/19_train.csv\",\"w\")\nrow=0\nfor i in f:\n    fw.write(i)\n    row+=1\n    if row==49950000:\n        break\nfw.close()\n\nfw=open(\"./data/eval_data/19_eval.csv\",\"w\")\nfor i in f:\n    fw.write(i)\nfw.close()\nf.close()","execution_count":null},{"metadata":{"id":"6178B3A6585649AB8EC202097B1C8F7B","mdEditEnable":false},"cell_type":"markdown","source":"# 针对前9亿数据和最后4995万数据，生成打乱后的数据，用于训练esim模型，后5万是验证集，18.csv本来打算用来融合模型，后来效果不好，就没用。"},{"metadata":{"id":"42453FF907404EC18EDBF675BF6137BA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import os\nfilename=os.listdir(\"/home/kesci/order_data\")\nfs=[open(\"/home/kesci/order_data/\"+f) for f in filename if f!=\"18.csv\" and f!=\"19.csv\"]\nfs.append(open(\"/home/kesci/19_train.csv\"))\nimport random\nfw=open(\"/home/kesci/shuffle_data.csv\",\"w\")\nwhile len(fs):\n    temp_index=random.randint(0,len(fs)-1) #每次随机从19份数据中\n    temp_str=fs[temp_index].readline()\n    if temp_str:\n        fw.write(temp_str)\n    else:\n        fs.remove(fs[temp_index])\nfw.close()","execution_count":null},{"metadata":{"id":"B3D34E550CCF430D9EB3AAB3C85D485B","mdEditEnable":false},"cell_type":"markdown","source":"# 统计所有词的文档频率，即所有词在所有title和query里面出现的次数，包括最终测试集和复赛第一个测试集"},{"metadata":{"id":"6F8F79B340CB478F8730C745CBE89743","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from collections import Counter\r\nimport pandas as pd\r\nimport gc\r\n\r\ndata_95 = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\ncount_set = set()\r\n\r\nfor line in data_95:\r\n    line = line.strip().split(',')\r\n    count_set.add(line[1])\r\n    count_set.add(line[3])\r\ndata_95.close()\r\n\r\ndata_95 = open(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\")\r\n\r\nfor line in data_95:\r\n    line = line.strip().split(',')\r\n    count_set.add(line[1])\r\n    count_set.add(line[3])\r\ndata_95.close()\r\n\r\ndata_95 = open(\"/home/kesci/input/bytedance/test_final_part1.csv\")\r\nfor line in data_95:\r\n    line = line.strip().split(',')\r\n    count_set.add(line[1])\r\n    count_set.add(line[3])\r\ndata_95.close()\r\nprint(len(count_set)) \r\ndoc_sum_fre=len(count_set) #记录文档的总数 237412321\r\nword_sens = {} #key是词，value是词出现过的文档数量\r\nfor ju in count_set:\r\n    temp_l = ju.split()\r\n    temp_s = set(temp_l)\r\n    for w in temp_s:\r\n        word_sens.setdefault(w, 0)\r\n        word_sens[w] += 1\r\nimport pickle\r\npickle.dump(word_sens,open(\"/home/kesci/work/data/word_sens.pkl\",\"wb\"))","execution_count":null},{"metadata":{"id":"1AE1EBCC65DF4AB087BCB6F6963EAEA1","mdEditEnable":false},"cell_type":"markdown","source":"# 用前9亿5000万数据，统计title的点击次数C与曝光次数I"},{"metadata":{"id":"6CCE05CEA8264BC487CBA5AADE10AEF1","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport os\nimport pickle\n\nsplit_data=\"/home/kesci/order_data\"\ndata_list=os.listdir(split_data)\ndata_list.remove(\"19.csv\")\n\nall_title_I={} #曝光次数\nall_title_C={} #点击次数\n\nfor d in data_list:\n    print(d)\n    d_index=d.split(\".\")[0]\n    \n    data_95=pd.read_csv(os.path.join(split_data,d),names=[\"query_id\",\"query\",\"title_id\",\"title\",\"label\"])\n    t_count=data_95[\"title\"].value_counts().to_dict()\n    t_click=data_95.groupby([\"title\"])[\"label\"].sum().to_dict()\n    for t in t_count:\n        all_title_I.setdefault(t,0)\n        all_title_I[t]+=t_count[t]\n    for t in t_click:\n        all_title_C.setdefault(t,0)\n        all_title_C[t]+=t_click[t]\n\npickle.dump(all_title_I,open(\"/home/kesci/all_title_I.pkl\",\"wb\"))\npickle.dump(all_title_C,open(\"/home/kesci/all_title_C.pkl\",\"wb\"))","execution_count":null},{"metadata":{"id":"7158B4E8EBBD41CDB3D8A4AFFF31DB76","mdEditEnable":false},"cell_type":"markdown","source":"# 用最后5000万数据统计，统计title的点击次数C与曝光次数I"},{"metadata":{"id":"90BEF09032A349F88E8968D1CF7F47B8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"all_title_I={}\nall_title_C={}\n\ndata_95=pd.read_csv(\"/home/kesci/order_data/19.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\",\"label\"])\nt_count=data_95[\"title\"].value_counts().to_dict()\nt_click=data_95.groupby([\"title\"])[\"label\"].sum().to_dict()\nfor t in t_count:\n    all_title_I.setdefault(t,0)\n    all_title_I[t]+=t_count[t]\nfor t in t_click:\n    all_title_C.setdefault(t,0)\n    all_title_C[t]+=t_click[t]\n    \npickle.dump(all_title_I,open(\"/home/kesci/title_I_19.pkl\",\"wb\"))\npickle.dump(all_title_C,open(\"/home/kesci/title_C_19.pkl\",\"wb\"))","execution_count":null},{"metadata":{"id":"FDEC66DE943C458BA93CCC99811A823A","mdEditEnable":false},"cell_type":"markdown","source":"# 用前9亿5000万数据统计，query和title里面每个词的曝光次数和点击次数"},{"metadata":{"id":"A26BB8A1524547FE8A77D19BFF94E6A2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import pickle\r\n\r\ntemp_q_dict = {}\r\nsum_q_dict = {}\r\ntemp_t_dict = {}\r\nsum_t_dict = {}\r\nf=open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nfor myindex,line in enumerate(f,1):\r\n    if myindex>950000000:\r\n        break\r\n    line = line.strip().split(\",\")\r\n    query = set(line[1].split())\r\n    title = set(line[3].split())\r\n    label = int(line[4])\r\n    for w in query:\r\n        temp_q_dict.setdefault(w, 0)\r\n        temp_q_dict[w] += label\r\n        sum_q_dict.setdefault(w, 0)\r\n        sum_q_dict[w] += 1\r\n    for w in title:\r\n        temp_t_dict.setdefault(w, 0)\r\n        temp_t_dict[w] += label\r\n        sum_t_dict.setdefault(w, 0)\r\n        sum_t_dict[w] += 1\r\n        \r\npickle.dump(temp_q_dict, open(\"temp_q_dict.pkl\",\"wb\"))\r\npickle.dump(sum_q_dict, open(\"sum_q_dict.pkl\", \"wb\"))\r\npickle.dump(temp_t_dict, open(\"temp_t_dict.pkl\", \"wb\"))\r\npickle.dump(sum_t_dict, open(\"sum_t_dict.pkl\", \"wb\"))\r\nf.close()","execution_count":null},{"metadata":{"id":"BA56144AFAA940B7931BEFBC5538B317","mdEditEnable":false},"cell_type":"markdown","source":"# 贝叶斯平滑title的ctr，以及query和title中词的ctr"},{"metadata":{"id":"124DE79992E54F7E870F11427A113D12","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import numpy\nimport scipy.special as special\nimport numpy as np\n\n\nclass BayesianSmoothing(object):\n    def __init__(self, alpha, beta):\n        self.alpha = alpha\n        self.beta = beta\n\n    def sample(self, alpha, beta, num, imp_upperbound):\n        sample = numpy.random.beta(alpha, beta, num)\n        print(sample)\n        I = []\n        C = []\n        for clk_rt in sample:\n            imp = imp_upperbound\n            clk = imp * clk_rt\n            I.append(imp)\n            C.append(clk)\n        return I, C\n\n    def update(self, imps, clks, iter_num, epsilon):\n        for i in range(iter_num):\n            new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n                break\n            self.alpha = new_alpha\n            self.beta = new_beta\n            print(\"第\"+str(i)+\"轮 alpha: \",self.alpha,\"beta: \",self.beta)\n\n    def __fixed_point_iteration(self, imps, clks, alpha, beta):\n        numerator_alpha = 0.0\n        numerator_beta = 0.0\n        denominator = 0.0\n\n        for i in range(len(imps)):\n            numerator_alpha += (special.digamma(clks[i] + alpha) - special.digamma(alpha))\n            numerator_beta += (special.digamma(imps[i] - clks[i] + beta) - special.digamma(beta))\n            denominator += (special.digamma(imps[i] + alpha + beta) - special.digamma(alpha + beta))\n\n        return alpha * (numerator_alpha / denominator), beta * (numerator_beta / denominator)\n\ndef get_init_alpha_beta(old_ctrs):\n    old_ctrs_mean=old_ctrs.mean()\n    print(\"ctr_mean:\",old_ctrs_mean)\n    old_ctrs_var=old_ctrs.var()\n    alpha = old_ctrs_mean/old_ctrs_var*(old_ctrs_mean*(1-old_ctrs_mean)-old_ctrs_var)\n    beta = (1-old_ctrs_mean)/old_ctrs_var*(old_ctrs_mean*(1-old_ctrs_mean)-old_ctrs_var)\n    return alpha,beta","execution_count":null},{"metadata":{"id":"E262B11A684D4220A2C798468736B405","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 贝叶斯平滑title的ctr\nimport pickle\nall_title_C=pickle.load(open(\"/home/kesci/all_title_C.pkl\",\"rb\"))\nall_title_I=pickle.load(open(\"/home/kesci/all_title_I.pkl\",\"rb\"))\nall_title_ctr={}\nfor t in all_title_I:\n    all_title_ctr[t]=all_title_C[t]/all_title_I[t]\n    \ndef smooth_ctr(old_ctrs,I_dict,C_dict,title_list,num_iters=100000,epsilon=0.0000000001):\n    I=[]\n    C=[]\n    for t in title_list:\n        I.append(I_dict[t])\n        C.append(C_dict[t])\n    init_alpha,init_beta=get_init_alpha_beta(np.array(list(old_ctrs.values()),dtype=np.float))\n    print(\"初始化参数：\",init_alpha,init_beta)\n    bs = BayesianSmoothing(init_alpha,init_beta)\n    bs.update(I,C,num_iters,epsilon)\n    print(\"收敛参数：alpha: \",bs.alpha,\"beta: \",bs.beta)\n\nsmooth_ctr(all_title_ctr,all_title_I,all_title_C,title_list=list(all_title_ctr.keys()),num_iters=200)\n# 由于时间花费的比较长，所以后面的代码用我跑了151轮的平滑参数\n# alpha=1.3912001443482402 beta=7.3260734359558795","execution_count":null},{"metadata":{"id":"B4F1A5F04207476F8F6C954AD5614567","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 贝叶斯平滑title中词的ctr\nimport pickle\nall_title_C=pickle.load(open(\"/home/kesci/work/temp_t_dict.pkl\",\"rb\"))\nall_title_I=pickle.load(open(\"/home/kesci/work/sum_t_dict.pkl\",\"rb\"))\nall_title_ctr={}\nfor t in all_title_I:\n    all_title_ctr[t]=all_title_C[t]/all_title_I[t]\nsmooth_ctr(all_title_ctr,all_title_I,all_title_C,title_list=list(all_title_ctr.keys()),num_iters=3321)\n# 由于时间花费的比较长，所以后面的代码用我跑了3321轮的平滑参数\n# alpha=9.316810724554587 beta=46.161204596782255","execution_count":null},{"metadata":{"id":"9D743629883C434380CAAE205D7B9FF3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import pickle\ncountdict = pickle.load(open(\"./data/word_sens.pkl\",\"rb\"))\nall_title_C=pickle.load(open(\"/home/kesci/work/temp_t_dict.pkl\",\"rb\"))\nall_title_I=pickle.load(open(\"/home/kesci/work/sum_t_dict.pkl\",\"rb\"))\nall_title_ctr={}\ndefault_ctr=9.316810724554587/(9.316810724554587+46.161204596782255)\nfor t in countdict:\n    if t in all_title_I:\n        all_title_ctr[t]=(all_title_C[t]+9.316810724554587)/(all_title_I[t]+9.316810724554587+46.161204596782255)\n    else:\n        all_title_ctr[t]=default_ctr\npickle.dump(all_title_ctr,open(\"/home/kesci/title_word_ctr.pkl\",\"wb\"))#每个title里面词的ctr","execution_count":null},{"metadata":{"id":"051B0BCF1ADA4A4FA3DB4335F86678B3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 贝叶斯平滑query中词的ctr\nimport pickle\nall_title_C=pickle.load(open(\"/home/kesci/work/temp_q_dict.pkl\",\"rb\"))\nall_title_I=pickle.load(open(\"/home/kesci/work/sum_q_dict.pkl\",\"rb\"))\nall_title_ctr={}\nfor t in all_title_I:\n    all_title_ctr[t]=all_title_C[t]/all_title_I[t]\nsmooth_ctr(all_title_ctr,all_title_I,all_title_C,title_list=list(all_title_ctr.keys()),num_iters=4717)\n# 由于时间花费的比较长，所以后面的代码用我跑了4717轮的平滑参数\n# alpha=23.805871972380245 beta=120.9641867898008\n\nimport pickle\ncountdict=pickle.load(open(\"./data/word_sens.pkl\",\"rb\"))\nall_title_C=pickle.load(open(\"/home/kesci/work/temp_q_dict.pkl\",\"rb\"))\nall_title_I=pickle.load(open(\"/home/kesci/work/sum_q_dict.pkl\",\"rb\"))\n\nall_title_ctr={}\ndefault_ctr=23.805871972380245/(23.805871972380245+120.9641867898008)\nfor t in countdict:\n    if t in all_title_I:\n        all_title_ctr[t]=(all_title_C[t]+23.805871972380245)/(all_title_I[t]+23.805871972380245+120.9641867898008)\n    else:\n        all_title_ctr[t]=default_ctr\npickle.dump(all_title_ctr,open(\"/home/kesci/query_word_ctr.pkl\",\"wb\"))#每个query里面词的ctr","execution_count":null},{"metadata":{"id":"7B7FFD3C2589451E97BBEC0576ABA4B1","mdEditEnable":false},"cell_type":"markdown","source":"# 统计最后5000万数据和最终1亿测试集的每个query和title的每个词的tfidf值"},{"metadata":{"id":"47235C53703C47BAB58062E58E8405DF","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from collections import Counter\nimport math\nimport pickle\n\ndef tf(word, count , line_len):\n    return count[word] / line_len\n\n\n# 统计的是含有该单词的句子数\ndef n_containing(word, count_list):\n    return sum(1 for count in count_list if word in count)\n\n\n# len(count_list)是指句子的总数，n_containing(word, count_list)是指含有该单词的句子的总数，加1是为了防止分母为0\ndef idf(word, word_sens,ju_sum):\n    return math.log(ju_sum / (1 + word_sens[word]))\n\n\n# 将tf和idf相乘\ndef tfidf(word,count,word_sens,line_len,ju_sum):\n    return tf(word,count,line_len) * idf(word,word_sens,ju_sum)\n    \nword_sens=pickle.load(open(\"./data/word_sens.pkl\",\"rb\"))\nju_sum=237412321\ndef get_tfidf(txt):\n    temp_l = txt.split()\n    res = \" \".join([str(tfidf(word, Counter(temp_l), word_sens, len(temp_l), ju_sum)) for word in temp_l])\n    return res\n\nimport pandas as pd\nimport pickle\ndata_list=[\"/home/kesci/order_data/19.csv\"]\n\nfor file in data_list:\n    print(file)\n    num=file.split(\"/\")[-1].split(\".\")[0]\n    print(num)\n    all_tfidf={}\n    dataset = pd.read_csv(file,names=[\"query_id\",\"query\",\"title_id\",\"title\",\"label\"],usecols=[\"query\",\"title\"])\n    for txt in dataset[\"query\"].unique():\n        if txt not in all_tfidf:\n            all_tfidf[txt]=get_tfidf(txt)\n    \n    for txt in dataset[\"title\"].unique():\n        if txt not in all_tfidf:\n            all_tfidf[txt]=get_tfidf(txt)\n    pickle.dump(all_tfidf,open(\"/home/kesci/all_txt_tfidf_\"+num+\".pkl\",\"wb\"))\n\ndata_list=[\"/home/kesci/test2.csv\"]\n\nfor file in data_list:\n    print(file)\n    num=file.split(\"/\")[-1].split(\".\")[0]\n    print(num)\n    all_tfidf={}\n    dataset = pd.read_csv(file,names=[\"query_id\",\"query\",\"title_id\",\"title\"],usecols=[\"query\",\"title\"])\n    for txt in dataset[\"query\"].unique():\n        if txt not in all_tfidf:\n            all_tfidf[txt]=get_tfidf(txt)\n    \n    for txt in dataset[\"title\"].unique():\n        if txt not in all_tfidf:\n            all_tfidf[txt]=get_tfidf(txt)\n    pickle.dump(all_tfidf,open(\"/home/kesci/all_txt_tfidf_\"+num+\".pkl\",\"wb\"))","execution_count":null},{"metadata":{"id":"DF20041531EE45639AF25D5F74E9EF3E","mdEditEnable":false},"cell_type":"markdown","source":"# 特征工程，对最后5000万和最终1亿测试集构造特征"},{"metadata":{"id":"61D87E7071CD4CFE96CB3763C1555329","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import pandas as pd\r\nimport numpy as np\r\nfrom fuzzywuzzy import fuzz\r\nfrom gensim.models import Word2Vec\r\nimport random\r\nimport Levenshtein\r\nfrom scipy.spatial.distance import minkowski, correlation, cityblock, braycurtis, cosine, euclidean,canberra,chebyshev","execution_count":null},{"metadata":{"id":"C70C552D3F924C6A9FF97AE9A42B630F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 得到子序列在query和title中的开始位置以及结束位置\r\ndef get_startendpos(value, length):\r\n\tquery_begin = 0\r\n\tquery_end = 0\r\n\ttitle_begin = 0\r\n\ttitle_end = 0\r\n\tchushi_v = 0\r\n\tcount = 0\r\n\tm, n = value.shape\r\n\tfor i in range(1, m):\r\n\t\tfor j in range(1, n):\r\n\t\t\tif int(value[i][j]) > chushi_v:\r\n\t\t\t\tif count == 0:\r\n\t\t\t\t\tquery_begin = i\r\n\t\t\t\t\ttitle_begin = j\r\n\t\t\t\tcount += 1\r\n\t\t\t\tchushi_v = int(value[i][j])\r\n\t\t\t\tif count == length:\r\n\t\t\t\t\tquery_end = i\r\n\t\t\t\t\ttitle_end = j\r\n\t\t\t\t\tbreak\r\n\treturn query_begin, query_end, title_begin, title_end","execution_count":null},{"metadata":{"id":"4733BC405BE14AD1B12C3F5DFC37BF70","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#得到最大公共子序列的长度，最长公共子序列在query中的跨度，最长公共子序列在title中的跨度，最长子序列长度/title长度\r\ndef get_max_same_xulie(query, title): \r\n\tq_list = query.split()\r\n\tquery_len = len(q_list)\r\n\tt_list = title.split()\r\n\ttitle_len = len(t_list)\r\n\tmy_value = np.zeros((query_len + 1, title_len + 1))\r\n\tfor i in range(1, query_len + 1):\r\n\t\tfor j in range(1, title_len + 1):\r\n\t\t\tif q_list[i - 1] == t_list[j - 1]:\r\n\t\t\t\tmy_value[i][j] = my_value[i - 1][j - 1] + 1\r\n\t\t\telse:\r\n\t\t\t\tmy_value[i][j] = max(my_value[i - 1][j], my_value[i][j - 1])\r\n\tmax_value = my_value.max()\r\n\tquery_begin, query_end, title_begin, title_end = get_startendpos(my_value, int(max_value))\r\n\treturn max_value, (query_end - query_begin) / query_len,(title_end - title_begin) / title_len, max_value / title_len","execution_count":null},{"metadata":{"id":"1A15188451DF4AC583B79C6E14B07B92","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 计算了query与title的最大相同子串长度，起始位置，平均位置，相同子串总长度，相同密度比，\r\n# 最大相同子串占query比，最大相同子串占title比\r\ndef get_son_str_feature(query, title):\r\n\tq_list = query.split()\r\n\tquery_len = len(q_list)\r\n\tt_list = title.split()\r\n\ttitle_len = len(t_list)\r\n\tcount1 = np.zeros((query_len + 1, title_len + 1))\r\n\tindex = np.zeros((query_len + 1, title_len + 1))\r\n\tfor i in range(1, query_len + 1):\r\n\t\tfor j in range(1, title_len + 1):\r\n\t\t\tif q_list[i - 1] == t_list[j - 1]:\r\n\t\t\t\tcount1[i][j] = count1[i - 1][j - 1] + 1\r\n\t\t\t\tindex[i][j] = index[i - 1][j - 1] + j\r\n\t\t\telse:\r\n\t\t\t\tcount1[i][j] = 0\r\n\t\t\t\tindex[i][j] = 0\r\n\tmax_count1 = count1.max()\r\n\r\n\tif max_count1 != 0:\r\n\t\trow = int(np.where(count1 == np.max(count1))[0][0])\r\n\t\tcol = int(np.where(count1 == np.max(count1))[1][0])\r\n\t\tmean_pos = index[row][col] / (max_count1 * title_len)\r\n\t\tbegin_loc = (col - max_count1 + 1) / title_len\r\n\t\trows = np.where(count1 != 0.0)[0]\r\n\t\tcols = np.where(count1 != 0.0)[1]\r\n\t\ttotal_loc = 0\r\n\t\tfor i in range(0, len(rows)):\r\n\t\t\ttotal_loc += index[rows[i]][cols[i]]\r\n\t\tdensity = total_loc / (query_len * title_len)\r\n\t\trate_q_len = max_count1 / query_len\r\n\t\trate_t_len = max_count1 / title_len\r\n\telse:\r\n\t\tbegin_loc, mean_pos, total_loc, density, rate_q_len, rate_t_len = 0, 0, 0, 0, 0, 0\r\n\treturn max_count1, begin_loc, mean_pos, total_loc, density, rate_q_len, rate_t_len","execution_count":2},{"metadata":{"id":"0FA78308432B4CB782BA7FD0BD4C6AA4","collapsed":false},"cell_type":"code","outputs":[],"source":"# 计算query和title中去重的相同词占所有单词的比例，\r\n# query在title中重合的词的长度与title在query中重合的词的长度的和除以query词的长度与title词的长度的和，\r\n# 相同词的长度占query长度比，\r\n# 相同词的长度占title长度比，\r\n# 每个相同词在query词中的平均位置，\r\n# 每个相同词在title词中的平均位置，\r\n# 去重的相同词占去重的query的长度比，\r\n# 去重的相同词占去重的title的长度比，\r\n# 两倍的去重的相同词的长度除以去重的title的词长度与去重的query的词长度的和，\r\n# 每个去重的相同词在去重的query词中的平均位置，\r\n# 每个去重的相同词在去重的title词中的平均位置，\r\n# 去重的相同词长度与今昔词的长度的比例\r\ndef get_same_word_features(query, title):\r\n\tq_list = query.split()\r\n\tt_list = title.split()\r\n\tset_query = set(q_list)\r\n\tset_title = set(t_list)\r\n\tcount_words = len(set_query.union(set_title))\r\n\r\n\tcomwords = [word for word in t_list if word in q_list]\r\n\tcomwords_set = set(comwords)\r\n\tunique_rate = len(comwords_set) / count_words\r\n\r\n\tsame_word1 = [w for w in q_list if w in t_list]\r\n\tsame_word2 = [w for w in t_list if w in q_list]\r\n\tsame_len_rate = (len(same_word1) + len(same_word2)) / (len(q_list) + len(t_list))\r\n\tif len(comwords) > 0:\r\n\t\tcom_index1 = len(comwords)\r\n\t\tsame_word_q = com_index1 / len(q_list)\r\n\t\tsame_word_t = com_index1 / len(t_list)\r\n\r\n\t\tfor word in comwords_set:\r\n\t\t\tindex_list = [i for i, x in enumerate(q_list) if x == word]\r\n\t\t\tcom_index1 += sum(index_list)\r\n\t\tq_loc = com_index1 / (len(q_list) * len(comwords))\r\n\t\tcom_index2 = len(comwords)\r\n\t\tfor word in comwords_set:\r\n\t\t\tindex_list = [i for i, x in enumerate(t_list) if x == word]\r\n\t\t\tcom_index2 += sum(index_list)\r\n\t\tt_loc = com_index2 / (len(t_list) * len(comwords))\r\n\r\n\t\tsame_w_set_q = len(comwords_set) / len(set_query)\r\n\t\tsame_w_set_t = len(comwords_set) / len(set_title)\r\n\t\tword_set_rate = 2 * len(comwords_set) / (len(set_query) + len(set_title))\r\n\r\n\t\tcom_set_query_index = len(comwords_set)\r\n\t\tfor word in comwords_set:\r\n\t\t\tindex_list = [i for i, x in enumerate(q_list) if x == word]\r\n\t\t\tif len(index_list) > 0:\r\n\t\t\t\tcom_set_query_index += index_list[0]\r\n\t\tloc_set_q = com_set_query_index / (len(q_list) * len(comwords_set))\r\n\t\tcom_set_title_index = len(comwords_set)\r\n\t\tfor word in comwords_set:\r\n\t\t\tindex_list = [i for i, x in enumerate(t_list) if x == word]\r\n\t\t\tif len(index_list) > 0:\r\n\t\t\t\tcom_set_title_index += index_list[0]\r\n\t\tloc_set_t = com_set_title_index / (len(t_list) * len(comwords_set))\r\n\t\tset_rate = (len(comwords_set) / len(comwords))\r\n\telse:\r\n\t\tunique_rate, same_len_rate, same_word_q, same_word_t, q_loc, t_loc, same_w_set_q, same_w_set_t, word_set_rate, loc_set_q, loc_set_t, set_rate = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\r\n\treturn unique_rate, same_len_rate, same_word_q, same_word_t, q_loc, t_loc, same_w_set_q, same_w_set_t, word_set_rate, loc_set_q, loc_set_t, set_rate","execution_count":3},{"metadata":{"id":"B05BC358D6D1484F8969D85D47DE1699","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from fuzzywuzzy import fuzz\r\nimport random\r\n# 因为是词别变成了数字编码，所以为了排除编码的影响，将每个编码映射成一个汉字，就是将每个词看成一个字，之后忽略\r\n# 顺序进行模糊匹配\r\ndef Unicode():\r\n    val=random.randint(0x4e00,0x9fbf)\r\n    return chr(val)\r\na=[]\r\nChinese=[]\r\nfor i in range(2000):\r\n    a.append(Unicode())\r\na=set(a)#随机生成的汉字去重\r\nfor word in a:#转成list\r\n    Chinese.append(word)\r\n\r\ndef get_fuzz_ratio1(df,model,reference):\r\n    #映射为汉字\r\n    query=df[model]\r\n    title=df[reference]\r\n    terms_model= query.split()\r\n    terms_reference= title.split()\r\n    grams_model = set(terms_model)\r\n    grams_reference = set(terms_reference)\r\n    Union=grams_model|grams_reference#讲两个set合并\r\n    query_chinese=[]\r\n    title_chinese=[]\r\n    Dict={}\r\n    count=0\r\n    for num in Union:\r\n        Dict[num]=Chinese[count]\r\n        count+=1\r\n    for num1 in terms_model:\r\n        query_chinese.append(Dict[num1])\r\n    for num2 in terms_reference:\r\n        title_chinese.append(Dict[num2])\r\n    str_query=' '\r\n    str_query=str_query.join(query_chinese)\r\n    str_title=' '\r\n    str_title=str_title.join(title_chinese)\r\n    token_sort_value = fuzz.token_sort_ratio(str_query, str_title)\r\n    return token_sort_value\r\n\r\n# 去重子集匹配\r\ndef get_fuzz_ratio2(df,model,reference):\r\n    #映射为汉字\r\n    query=df[model]\r\n    title=df[reference]\r\n    terms_model= query.split()\r\n    terms_reference= title.split()\r\n    grams_model = set(terms_model)\r\n    grams_reference = set(terms_reference)\r\n    Union=grams_model|grams_reference#讲两个set合并\r\n    query_chinese=[]\r\n    title_chinese=[]\r\n    Dict={}\r\n    count=0\r\n    for num in Union:\r\n        Dict[num]=Chinese[count]\r\n        count+=1\r\n    for num1 in terms_model:\r\n        query_chinese.append(Dict[num1])\r\n    for num2 in terms_reference:\r\n        title_chinese.append(Dict[num2])\r\n    str_query=' '\r\n    str_query=str_query.join(query_chinese)\r\n    str_title=' '\r\n    str_title=str_title.join(title_chinese)\r\n    token_set_value = fuzz.token_set_ratio(str_query, str_title)\r\n    return token_set_value","execution_count":4},{"metadata":{"id":"37FE996C29B144CEA38A0EB7EA10C891","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 结合tfidf与余弦相似度，将词频换成tfidf值，计算query和title的余弦相似度\ndef cos_dist(vec1, vec2):\n    \"\"\"\n    :param vec1: 向量1\n    :param vec2: 向量2\n    :return: 返回两个向量的余弦相似度\n    \"\"\"\n    dist1 = float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n    return dist1\n\ndef get_tfidf_scores(df, s1, s2):\n    \"\"\"\n    :param s1: 句子1\n    :param s2: 句子2\n    :return: 返回句子的余弦相似度\n    \"\"\"\n    # 分词\n\n    list_word1 = df[s1].split()\n    list_word2 = df[s2].split()\n    tfidf_score1 = {i[0]: float(i[1]) for i in zip(list_word1, all_tfidf[df[s1]].split())}\n    tfidf_score2 = {i[0]: float(i[1]) for i in zip(list_word2, all_tfidf[df[s2]].split())}\n\n    # 列出所有的词,取并集\n    key_word = list(set(list_word1 + list_word2))\n\n    # 给定形状和类型的用0填充的矩阵存储向量\n    word_vector1 = np.zeros(len(key_word))\n    word_vector2 = np.zeros(len(key_word))\n\n    # 计算词频\n    # 依次确定向量的每个位置的值\n    for i in range(len(key_word)):\n        # 遍历key_word中每个词在句子中的出现次数\n        for j in range(len(list_word1)):\n            if key_word[i] == list_word1[j]:\n                word_vector1[i] += tfidf_score1[key_word[i]]\n        for k in range(len(list_word2)):\n            if key_word[i] == list_word2[k]:\n                word_vector2[i] += tfidf_score2[key_word[i]]\n    return cos_dist(word_vector1, word_vector2)","execution_count":null},{"metadata":{"id":"7FCC1F25BF174F4AA7A70736FF9B0C25","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 提取query与title中互不重复的独立词\r\ndef extr_words(str_query, str_title):\r\n    list_query = str_query.split()\r\n    list_title = str_title.split()\r\n\r\n    query_word = [word for word in list_query if word not in list_title]\r\n    title_word = [word for word in list_title if word not in list_query]\r\n\r\n    return query_word, title_word\r\n\r\n# 提取query和title中所有词，加载词向量，将query和title所有词的词向量分别相加后求平均，\r\n# 得到的query平均词向量和title平均词向量进行多种距离函数计算其平均词向量相似度\r\ndef get_sim(str_query, str_title):\r\n    list_query = str_query.split()\r\n    list_title = str_title.split()\r\n    query = []\r\n    for word in list_query:\r\n        if word_vectors.__contains__(word):\r\n            query.append(word_vectors.__getitem__(word))\r\n    query = np.array(query)\r\n    if len(query) != 0:\r\n        q_vec = query.sum(axis=0) / query.shape[0]\r\n    else:\r\n        q_vec = np.zeros((128,), dtype=np.float32)\r\n\r\n    title = []\r\n    for word in list_title:\r\n        if word_vectors.__contains__(word):\r\n            title.append(word_vectors.__getitem__(word))\r\n    title = np.array(title)\r\n    if len(title) != 0:\r\n        t_vec = title.sum(axis=0) / title.shape[0]\r\n    else:\r\n        t_vec = np.zeros((128,), dtype=np.float32)\r\n    dot_prod = np.sum(np.multiply(q_vec, t_vec))  \r\n    braycurtis_val = braycurtis(q_vec, t_vec)\r\n    canberra_val = canberra(q_vec, t_vec)\r\n    euclidean_val = euclidean(q_vec, t_vec)\r\n    minkowski_val = minkowski(q_vec, t_vec, p=3)\r\n    cityblock_val = cityblock(q_vec, t_vec)\r\n    correlation_val = correlation(q_vec, t_vec)\r\n    cosine_val = cosine(q_vec, t_vec)\r\n    chebyshev_val = chebyshev(q_vec, t_vec)\r\n    if cosine_val == 'nan' or cosine_val == 'inf':\r\n        cosine_val = 2\r\n    if braycurtis_val == 'nan' or braycurtis_val == 'inf':\r\n        braycurtis_val = 2\r\n    if correlation_val == 'nan' or correlation_val == 'inf':\r\n        correlation_val = 1\r\n\r\n    return dot_prod, braycurtis_val, canberra_val, euclidean_val, minkowski_val, cityblock_val, correlation_val, cosine_val, chebyshev_val","execution_count":null},{"metadata":{"id":"2E4DD42979D6473DA1E306983C5C8C40","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 提取query和title中互不相同的独立词，\r\n# 加载词向量，将query和title独立词的词向量分别相加后求平均，\r\n# 得到的query平均独立词向量和title平均独立词向量进行多种距离函数计算其平均独立词向量距离\r\ndef get_dis(str_query, str_title):\r\n    list_query, list_title = extr_words(str_query, str_title)\r\n    query = []\r\n    for word in list_query:\r\n        if word_vectors.__contains__(word):\r\n            query.append(word_vectors.__getitem__(word))\r\n    query = np.array(query)\r\n    if len(query) != 0:\r\n        q_vec = query.sum(axis=0) / query.shape[0]\r\n    else:\r\n        q_vec = np.zeros((128,), dtype=np.float32)\r\n    title = []\r\n    for word in list_title:\r\n        if word_vectors.__contains__(word):\r\n            title.append(word_vectors.__getitem__(word))\r\n    title = np.array(title)\r\n    if len(title) != 0:\r\n        t_vec = title.sum(axis=0) / title.shape[0]\r\n    else:\r\n        t_vec = np.zeros((128,), dtype=np.float32)\r\n    dis_dot_prod = np.sum(np.multiply(q_vec, t_vec))  \r\n    dis_braycurtis_val = braycurtis(q_vec, t_vec)\r\n    dis_euclidean_val = euclidean(q_vec, t_vec)\r\n    dis_minkowski_val = minkowski(q_vec, t_vec, p=3)\r\n    dis_canberra_val = canberra(q_vec, t_vec)\r\n    dis_cityblock_val = cityblock(q_vec, t_vec)\r\n    dis_correlation_val = correlation(q_vec, t_vec)\r\n    dis_cosine_val = cosine(q_vec, t_vec)\r\n    dis_chebyshev_val = chebyshev(q_vec, t_vec)\r\n\r\n    if dis_cosine_val == 'nan' or dis_cosine_val == 'inf':\r\n        dis_cosine_val = 2\r\n    if dis_braycurtis_val == 'nan' or dis_braycurtis_val == 'inf':\r\n        dis_braycurtis_val = 2\r\n    if dis_correlation_val == 'nan' or dis_correlation_val == 'inf':\r\n        dis_correlation_val = 1\r\n    return dis_dot_prod, dis_braycurtis_val, dis_euclidean_val, dis_minkowski_val, dis_canberra_val, dis_cityblock_val, dis_correlation_val, dis_cosine_val, dis_chebyshev_val","execution_count":null},{"metadata":{"id":"6D7B826A3F65480C82049EF009D692A4","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 因为是词别变成了数字编码，所以为了排除编码的影响，将每个编码映射成一个汉字，就是将每个词看成一个字。\n# 计算莱文斯坦比。\n# 计算公式  r = (sum - ldist) / sum, 其中sum是指str1 和 str2 字串的长度总和，ldist是 类编辑距离\ndef cal_ratio(df,model,reference):\n    #映射为汉字\n    query=df[model]\n    title=df[reference]\n    terms_model= query.split()\n    terms_reference= title.split()\n    grams_model = set(terms_model)\n    grams_reference = set(terms_reference)\n    Union=grams_model|grams_reference#讲两个set合并\n    query_chinese=[]\n    title_chinese=[]\n    Dict={}\n    count=0\n    for num in Union:\n        Dict[num]=Chinese[count]\n        count+=1\n    for num1 in terms_model:\n        query_chinese.append(Dict[num1])\n    for num2 in terms_reference:\n        title_chinese.append(Dict[num2])\n    str_query=''\n    str_query=str_query.join(query_chinese)\n    str_title=''\n    str_title=str_title.join(title_chinese)\n    ratio=Levenshtein.ratio(str_query,str_title)\n    return ratio\n\n# 计算query和title的jaro距离\ndef cal_chinese_jaro(df,model,reference):\n    #映射为汉字\n    query=df[model]\n    title=df[reference]\n    terms_model= query.split()\n    terms_reference= title.split()\n    grams_model = set(terms_model)\n    grams_reference = set(terms_reference)\n    Union=grams_model|grams_reference#讲两个set合并\n    query_chinese=[]\n    title_chinese=[]\n    Dict={}\n    count=0\n    for num in Union:\n        Dict[num]=Chinese[count]\n        count+=1\n    for num1 in terms_model:\n        query_chinese.append(Dict[num1])\n    for num2 in terms_reference:\n        title_chinese.append(Dict[num2])\n    str_query=''\n    str_query=str_query.join(query_chinese)\n    str_title=''\n    str_title=str_title.join(title_chinese)\n    ratio=Levenshtein.jaro(str_query,str_title)\n    return ratio\n\n# 计算query和title的Jaro–Winkler距离\ndef cal_chinese_jaro_winkler(df,model,reference):\n    #映射为汉字\n    query=df[model]\n    title=df[reference]\n    terms_model= query.split()\n    terms_reference= title.split()\n    grams_model = set(terms_model)\n    grams_reference = set(terms_reference)\n    Union=grams_model|grams_reference#讲两个set合并\n    query_chinese=[]\n    title_chinese=[]\n    Dict={}\n    count=0\n    for num in Union:\n        Dict[num]=Chinese[count]\n        count+=1\n    for num1 in terms_model:\n        query_chinese.append(Dict[num1])\n    for num2 in terms_reference:\n        title_chinese.append(Dict[num2])\n    str_query=''\n    str_query=str_query.join(query_chinese)\n    str_title=''\n    str_title=str_title.join(title_chinese)\n    ratio=Levenshtein.jaro_winkler(str_query,str_title)\n    return ratio","execution_count":null},{"metadata":{"id":"DC7C99A51FB64A9C801B45072F1108F9","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 计算query和title的BM25相关性\nimport math\ndef calBM25(data1,col1,col2):\n    querytemp=data1[col1].split()\n    titletemp=data1[col2].split()\n    dict_temp={}\n    for word in querytemp:\n        if word in dict_temp:\n            dict_temp[word]+=1\n        else:\n            dict_temp[word]=1\n    sumscore=0\n    sl=len(titletemp)\n    for word in titletemp: \n        if word in countdict and word in dict_temp:\n            fi=dict_temp[word]\n            sumscore+=math.log(237412321 / (1 + countdict[word]))*fi*(2+1)/(fi+2*(1-0.75+0.75*sl/10))\n    return sumscore","execution_count":5},{"metadata":{"id":"26DAE90174E64492855936D2F03856BC","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#计算query和title的2-gram和3-gram，相同数\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os\n\nclass StringDistance:\n\n    def distance(self, s0, s1):\n        raise NotImplementedError()\n\n\nclass NormalizedStringDistance(StringDistance):\n\n    def distance(self, s0, s1):\n        raise NotImplementedError()\n\n\nclass MetricStringDistance(StringDistance):\n\n    def distance(self, s0, s1):\n        raise NotImplementedError()\n\nclass NGram(NormalizedStringDistance):\n\n    def __init__(self, n=2):\n        self.n = n\n\n    def distance(self, s0, s1):\n        if s0 is None:\n            raise TypeError(\"Argument s0 is NoneType.\")\n        if s1 is None:\n            raise TypeError(\"Argument s1 is NoneType.\")\n        if s0 == s1:\n            return 0.0\n\n        special = '\\n'\n        sl = len(s0)\n        tl = len(s1)\n\n        if sl == 0 or tl == 0:\n            return 1.0\n\n        cost = 0\n        if sl < self.n or tl < self.n:\n            for i in range(min(sl, tl)):\n                if s0[i] == s1[i]:\n                    cost += 1\n            return 1.0 * cost / max(sl, tl)\n\n        sa = [''] * (sl + self.n - 1)\n\n        for i in range(len(sa)):\n            if i < self.n - 1:\n                sa[i] = special\n            else:\n                sa[i] = s0[i - self.n + 1]\n\n        p = [0.0] * (sl + 1)\n        d = [0.0] * (sl + 1)\n        t_j = [''] * self.n\n        for i in range(sl + 1):\n            p[i] = 1.0 * i\n\n        for j in range(1, tl + 1):\n            if j < self.n:\n                for ti in range(self.n - j):\n                    t_j[ti] = special\n                for ti in range(self.n - j, self.n):\n                    t_j[ti] = s1[ti - (self.n - j)]\n            else:\n                t_j = s1[j - self.n:j]\n\n            d[0] = 1.0 * j\n            for i in range(sl + 1):\n                cost = 0\n                tn = self.n\n                for ni in range(self.n):\n                    if sa[i - 1 + ni] != t_j[ni]:\n                        cost += 1\n                    elif sa[i - 1 + ni] == special:\n                        tn -= 1\n                ec = cost / tn\n                d[i] = min(d[i - 1] + 1, p[i] + 1, p[i - 1] + ec)\n            p, d = d, p\n\n        return p[sl] / max(tl, sl)\ntwogram = NGram(2)\ntrigram = NGram(3)\n\ndef two_gram(df,col1,col2):\n    Query=df[col1].split()\n    Title=df[col2].split()\n    Two_gram=twogram.distance(Query,Title)\n    return Two_gram\n\ndef tri_gram(df,col1,col2):\n    Query=df[col1].split()\n    Title=df[col2].split()\n    Tri_gram=trigram.distance(Query,Title)\n    return Tri_gram","execution_count":6},{"metadata":{"id":"238FCC02118347A788158AA822F23A6D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#计算query和title的编辑距离\ndef edit_distance(df, w1, w2):\n    word1 = df[w1].split()\n    word2 = df[w2].split()\n    len1 = len(word1)\n    len2 = len(word2)\n    dp = np.zeros((len1 + 1, len2 + 1))\n    for i in range(len1 + 1):\n        dp[i][0] = i\n    for j in range(len2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len1 + 1):\n        for j in range(1, len2 + 1):\n            delta = 0 if word1[i - 1] == word2[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j - 1] + delta, min(dp[i - 1][j] + 1, dp[i][j - 1] + 1))\n    return dp[len1][len2]","execution_count":7},{"metadata":{"id":"C5F0E50BE65A422DAAC7E485949EC76A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 得到query中tfidf最大的那个词的ctr\ndef get_best_query_word_ctr(txt):\n    all_w=txt.split()\n    tfidf_score1 = {i[0]: float(i[1]) for i in zip(all_w, all_tfidf[txt].split())}\n    p_d=sorted(tfidf_score1.items(),key=lambda item:item[1],reverse=True)\n    return query_word_ctr[p_d[0][0]]\n  \n# 得到title中tfidf最大的那个词的ctr \ndef get_best_title_word_ctr(txt):\n    all_w=txt.split()\n    tfidf_score1 = {i[0]: float(i[1]) for i in zip(all_w, all_tfidf[txt].split())}\n    p_d=sorted(tfidf_score1.items(),key=lambda item:item[1],reverse=True)\n    return title_word_ctr[p_d[0][0]]","execution_count":8},{"metadata":{"id":"852CFDBF852C466181196A83FC7BCCD8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 得到query的每个词的tfidf值和每个词的ctr的加权和。\ndef get_sum_query_word_ctr(txt):\n    all_w=txt.split()\n    tfidf_score=np.array([float(i) for i in all_tfidf[txt].split()])\n    ctr_score=np.array([query_word_ctr[i] for i in all_w])\n    return np.dot(tfidf_score,ctr_score)\n\n# 得到title的每个词的tfidf值和每个词的ctr的加权和。\ndef get_sum_title_word_ctr(txt):\n    all_w=txt.split()\n    tfidf_score=np.array([float(i) for i in all_tfidf[txt].split()])\n    ctr_score=np.array([title_word_ctr[i] for i in all_w])\n    return np.dot(tfidf_score,ctr_score)","execution_count":9},{"metadata":{"id":"CBEFAE92BEF64F0694622D44333CFFB8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"data = pd.read_csv(\"/home/kesci/order_data/19.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\",\"label\"])\n\n\ndata[\"long_same_lie\"],data[\"long_same_lie_q\"],data[\"long_same_lie_t\"],data[\"long_same_t_rate\"]=zip(*data.apply(lambda line: get_max_same_xulie(line[\"query\"], line[\"title\"]),axis=1))\ndata[[\"long_same_lie\",\"long_same_lie_q\",\"long_same_lie_t\",\"long_same_t_rate\"]].to_csv(\n    \"/home/kesci/work/features/train/long_same_lie.csv\",index=False)\n\n\ndata[\"long_same_max_count1\"], data[\"long_same_local_begin\"], data[\"long_same_local_mean\"],data[\"long_same_total_loc\"],\\\ndata[\"long_same_density\"], data[\"long_same_rate_q_len\"], data[\"long_same_rate_t_len\"]= zip(\n    *data.apply(lambda line: get_son_str_feature(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"long_same_max_count1\",\"long_same_local_begin\",\"long_same_local_mean\",\"long_same_total_loc\",\n\"long_same_density\",\"long_same_rate_q_len\",\"long_same_rate_t_len\"]].to_csv(\n    \"/home/kesci/work/features/train/long_same_str.csv\",index=False)\n\n\ndata[\"unique_rate\"],data[\"same_len_rate\"],data[\"same_word_q\"],\\\ndata[\"same_word_t\"],data[\"q_loc\"],data[\"t_loc\"],data[\"same_w_set_q\"],data[\"same_w_set_t\"],data[\"word_set_rate\"],\\\ndata[\"loc_set_q\"], data[\"loc_set_t\"], data[\"set_rate\"]= zip(\n    *data.apply(lambda line: get_same_word_features(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"unique_rate\", \"same_len_rate\", \"same_word_q\", \"same_word_t\", \"q_loc\", \"t_loc\", \"same_w_set_q\", \"same_w_set_t\", \"word_set_rate\", \"loc_set_q\", \"loc_set_t\", \"set_rate\"]].to_csv(\n    \"/home/kesci/work/features/train/same_w_q_t.csv\",index=False)\n\n\ndata['fuzz_1']=data.apply(get_fuzz_ratio1,axis=1,args=('query','title'))\ndata['fuzz_2']=data.apply(get_fuzz_ratio2,axis=1,args=('query','title'))\ndata[[\"fuzz_1\",\"fuzz_2\"]].to_csv(\n    \"/home/kesci/work/features/train/fuzz.csv\",index=False)\n\n\nimport pickle\nall_tfidf=pickle.load(open(\"/home/kesci/all_txt_tfidf_19.pkl\",\"rb\"))\ndata[\"cos_tfidf_distance\"] = data.apply(get_tfidf_scores,axis = 1,args = ('query','title'))\ndata[[\"cos_tfidf_distance\"]].to_csv(\"/home/kesci/work/features/train/cos_tfidf_distance.csv\",index=False)\n\n\nmodel = Word2Vec.load(\"/home/kesci/word2vec.model\")\nword_vectors = model.wv\ndel model\ndata[\"sim_1\"], data[\"sim_2\"], data[\"sim_3\"], data[\"sim_4\"], data[\"sim_5\"],data[\"sim_6\"], \\\ndata[\"sim_7\"],data[\"sim_8\"],data[\"sim_9\"]= zip(\n    *data.apply(lambda line: get_sim(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"sim_1\",\"sim_2\",\"sim_3\",\"sim_4\",\"sim_5\",\"sim_6\",\"sim_7\",\"sim_8\",\"sim_9\"]].fillna(0).to_csv(\n    \"/home/kesci/work/features/train/sim_feature.csv\",\nindex=False)\n\n\ndata[\"dis_1\"], data[\"dis_2\"], data[\"dis_3\"], data[\"dis_4\"],data[\"dis_5\"], data[\"dis_6\"], data[\"dis_7\"],\\\ndata[\"dis_8\"],data[\"dis_9\"] = zip(\n    *data.apply(lambda line: get_dis(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"dis_1\",\"dis_2\",\"dis_3\",\"dis_4\",\"dis_5\",\"dis_6\",\"dis_7\",\"dis_8\",\"dis_9\"]].fillna(0).to_csv(\n    \"/home/kesci/work/features/train/dis_feature.csv\",index=False)\n\n\ndata['Chinese_ratio']=data.apply(cal_ratio,axis=1,args=('query','title'))\ndata['jaro']=data.apply(cal_chinese_jaro,axis=1,args=('query','title'))\ndata['jaro_winkler']=data.apply(cal_chinese_jaro_winkler,axis=1,args=('query','title'))\ndata[[\"Chinese_ratio\",\"jaro\",\"jaro_winkler\"]].to_csv(\"/home/kesci/work/features/train/Levenshtein.csv\",index=False)\n\n\nimport pickle\ncountdict=pickle.load(open(\"./data/word_sens.pkl\",\"rb\"))\ndata['BM25'] = data.apply(calBM25,axis=1,args=('query','title'))\ndata[[\"BM25\"]].to_csv(\"/home/kesci/work/features/train/bm25.csv\",index=False)\n\n\ndata['two_gram']=data.apply(two_gram,axis=1,args=('query','title'))\ndata['tri_gram']=data.apply(tri_gram,axis=1,args=('query','title'))\ndata[['two_gram','tri_gram']].to_csv(\"/home/kesci/work/features/train/gram.csv\",index=False)\n\n\ndata[\"edit_distance\"] = data.apply(edit_distance, axis=1, args=('query', 'title'))\ndata[[\"edit_distance\"]].to_csv(\"/home/kesci/work/features/train/edit_distance.csv\",index=False)\n\n\n# 计算query和title的长度\ndata[\"query_len\"]=data[\"query\"].apply(lambda x:len(x.split()))\ndata[\"title_len\"]=data[\"title\"].apply(lambda x:len(x.split()))\ndata[[\"query_len\",\"title_len\"]].to_csv(\"/home/kesci/work/features/train/query_and_title_len.csv\",index=False)\n\n\ndata[\"best_query_word_ctr\"] = data[\"query\"].apply(get_best_query_word_ctr)\ndata[\"best_title_word_ctr\"] = data[\"title\"].apply(get_best_title_word_ctr)\ndata[\"sum_query_word_ctr\"] = data[\"query\"].apply(get_sum_query_word_ctr)\ndata[\"sum_title_word_ctr\"] = data[\"title\"].apply(get_sum_title_word_ctr)\ndata[[\"best_query_word_ctr\",\"best_title_word_ctr\",\"sum_query_word_ctr\",\"sum_title_word_ctr\"]].to_csv(\n    \"/home/kesci/work/features/train/query_title_word_ctr.csv\",\nindex=False)\n\n\n# 利用最后5000万数据进行，5折交叉统计title的ctr\ndata[\"5_ctr\"]=None\nalpha=1.3912001443482402\nbeta=7.3260734359558795\nctr_5=[]\nfor i in range(5):\n    if i==0:\n        data_95=data.loc[10000000:]\n    elif i==4:\n        data_95=data.loc[:4*10000000-1]\n    else:\n        data_95=pd.concat([data.loc[:i*10000000-1],data.loc[(i+1)*10000000:]],axis=0)\n    all_title_I={}\n    all_title_C={}\n    t_count=data_95[\"title\"].value_counts().to_dict()\n    t_click=data_95.groupby([\"title\"])[\"label\"].sum().to_dict()\n    for t in t_count:\n        all_title_I[t]=t_count[t]\n    for t in t_click:\n        all_title_C[t]=t_click[t]\n    def get_5_ctr(title):\n        if title in all_title_I:\n            return (all_title_C[title]+alpha)/(all_title_I[title]+alpha+beta)\n        else:\n            return alpha/(alpha+beta)\n    ctr_5.extend(list(data.loc[i*10000000:(i+1)*10000000-1][\"title\"].apply(get_5_ctr)))\ndata[\"5_ctr\"]=ctr_5\ndata[[\"5_ctr\"]].to_csv(\"/home/kesci/work/features/train/ctr_5.csv\",index=False)","execution_count":10},{"metadata":{"id":"D5D2D0C735604597859623DA57DAA525","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"data = pd.read_csv(\"/home/kesci/test2.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\"])\n\n\ndata[\"long_same_lie\"],data[\"long_same_lie_q\"],data[\"long_same_lie_t\"],data[\"long_same_t_rate\"]=zip(*data.apply(lambda line: get_max_same_xulie(line[\"query\"], line[\"title\"]),axis=1))\ndata[[\"long_same_lie\",\"long_same_lie_q\",\"long_same_lie_t\",\"long_same_t_rate\"]].to_csv(\n    \"/home/kesci/work/features/test2/long_same_lie.csv\",index=False)\n\n\ndata[\"long_same_max_count1\"], data[\"long_same_local_begin\"], data[\"long_same_local_mean\"],data[\"long_same_total_loc\"],\\\ndata[\"long_same_density\"], data[\"long_same_rate_q_len\"], data[\"long_same_rate_t_len\"]= zip(\n    *data.apply(lambda line: get_son_str_feature(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"long_same_max_count1\",\"long_same_local_begin\",\"long_same_local_mean\",\"long_same_total_loc\",\n\"long_same_density\",\"long_same_rate_q_len\",\"long_same_rate_t_len\"]].to_csv(\n    \"/home/kesci/work/features/test2/long_same_str.csv\",index=False)\n\n\ndata[\"unique_rate\"],data[\"same_len_rate\"],data[\"same_word_q\"],\\\ndata[\"same_word_t\"],data[\"q_loc\"],data[\"t_loc\"],data[\"same_w_set_q\"],data[\"same_w_set_t\"],data[\"word_set_rate\"],\\\ndata[\"loc_set_q\"], data[\"loc_set_t\"], data[\"set_rate\"]= zip(\n    *data.apply(lambda line: get_same_word_features(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"unique_rate\", \"same_len_rate\", \"same_word_q\", \"same_word_t\", \"q_loc\", \"t_loc\", \"same_w_set_q\", \"same_w_set_t\", \"word_set_rate\", \"loc_set_q\", \"loc_set_t\", \"set_rate\"]].to_csv(\n    \"/home/kesci/work/features/test2/same_w_q_t.csv\",index=False)\n\n\ndata['fuzz_1']=data.apply(get_fuzz_ratio1,axis=1,args=('query','title'))\ndata['fuzz_2']=data.apply(get_fuzz_ratio2,axis=1,args=('query','title'))\ndata[[\"fuzz_1\",\"fuzz_2\"]].to_csv(\n    \"/home/kesci/work/features/test2/fuzz.csv\",index=False)\n\n\nimport pickle\nall_tfidf=pickle.load(open(\"/home/kesci/all_txt_tfidf_test2.pkl\",\"rb\"))\ndata[\"cos_tfidf_distance\"] = data.apply(get_tfidf_scores,axis = 1,args = ('query','title'))\ndata[[\"cos_tfidf_distance\"]].to_csv(\"/home/kesci/work/features/test2/cos_tfidf_distance.csv\",index=False)\n\n\nmodel = Word2Vec.load(\"/home/kesci/word2vec.model\")\nword_vectors = model.wv\ndel model\ndata[\"sim_1\"], data[\"sim_2\"], data[\"sim_3\"], data[\"sim_4\"], data[\"sim_5\"],data[\"sim_6\"], \\\ndata[\"sim_7\"],data[\"sim_8\"],data[\"sim_9\"]= zip(\n    *data.apply(lambda line: get_sim(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"sim_1\",\"sim_2\",\"sim_3\",\"sim_4\",\"sim_5\",\"sim_6\",\"sim_7\",\"sim_8\",\"sim_9\"]].fillna(0).to_csv(\n    \"/home/kesci/work/features/test2/sim_feature.csv\",\nindex=False)\n\n\ndata[\"dis_1\"], data[\"dis_2\"], data[\"dis_3\"], data[\"dis_4\"],data[\"dis_5\"], data[\"dis_6\"], data[\"dis_7\"],\\\ndata[\"dis_8\"],data[\"dis_9\"] = zip(\n    *data.apply(lambda line: get_dis(line[\"query\"], line[\"title\"]), axis=1))\ndata[[\"dis_1\",\"dis_2\",\"dis_3\",\"dis_4\",\"dis_5\",\"dis_6\",\"dis_7\",\"dis_8\",\"dis_9\"]].fillna(0).to_csv(\n    \"/home/kesci/work/features/test2/dis_feature.csv\",index=False)\n\n\ndata['Chinese_ratio']=data.apply(cal_ratio,axis=1,args=('query','title'))\ndata['jaro']=data.apply(cal_chinese_jaro,axis=1,args=('query','title'))\ndata['jaro_winkler']=data.apply(cal_chinese_jaro_winkler,axis=1,args=('query','title'))\ndata[[\"Chinese_ratio\",\"jaro\",\"jaro_winkler\"]].to_csv(\"/home/kesci/work/features/test2/Levenshtein.csv\",index=False)\n\n\nimport pickle\ncountdict=pickle.load(open(\"./data/word_sens.pkl\",\"rb\"))\ndata['BM25'] = data.apply(calBM25,axis=1,args=('query','title'))\ndata[[\"BM25\"]].to_csv(\"/home/kesci/work/features/test2/bm25.csv\",index=False)\n\n\ndata['two_gram']=data.apply(two_gram,axis=1,args=('query','title'))\ndata['tri_gram']=data.apply(tri_gram,axis=1,args=('query','title'))\ndata[['two_gram','tri_gram']].to_csv(\"/home/kesci/work/features/test2/gram.csv\",index=False)\n\n\ndata[\"edit_distance\"] = data.apply(edit_distance, axis=1, args=('query', 'title'))\ndata[[\"edit_distance\"]].to_csv(\"/home/kesci/work/features/test2/edit_distance.csv\",index=False)\n\n\n# 计算query和title的长度\ndata[\"query_len\"]=data[\"query\"].apply(lambda x:len(x.split()))\ndata[\"title_len\"]=data[\"title\"].apply(lambda x:len(x.split()))\ndata[[\"query_len\",\"title_len\"]].to_csv(\"/home/kesci/work/features/test2/query_and_title_len.csv\",index=False)\n\n\ndata[\"best_query_word_ctr\"] = data[\"query\"].apply(get_best_query_word_ctr)\ndata[\"best_title_word_ctr\"] = data[\"title\"].apply(get_best_title_word_ctr)\ndata[\"sum_query_word_ctr\"] = data[\"query\"].apply(get_sum_query_word_ctr)\ndata[\"sum_title_word_ctr\"] = data[\"title\"].apply(get_sum_title_word_ctr)\ndata[[\"best_query_word_ctr\",\"best_title_word_ctr\",\"sum_query_word_ctr\",\"sum_title_word_ctr\"]].to_csv(\n    \"/home/kesci/work/features/test2/query_title_word_ctr.csv\",\nindex=False)","execution_count":null},{"metadata":{"id":"EE6D14F0DD7C4EA59DF2B41FCF43B66C","mdEditEnable":false},"cell_type":"markdown","source":"# 多维特征训练lgb模型，含有部分特征构造(因为构造较快，所以不需要提前做好存储)"},{"metadata":{"id":"C46A5CA4DDFD495894D0574DFB83E5F5","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import lightgbm as lgb\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\nimport gc\r\nfrom sklearn.model_selection import train_test_split","execution_count":null},{"metadata":{"id":"0821904DFA0F4AFC96174F4A63395C7C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"data_5 = pd.read_csv(\"/home/kesci/order_data/19.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\",\"label\"])","execution_count":null},{"metadata":{"id":"62C7D462B9A546998A61A340F66D4A5A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 每个query对应不同的title数量\r\n# 每个title对应不同的query数量\r\ndata_5[\"query_title_count\"] = data_5.groupby(\"query\")[\"title\"].transform(\"nunique\")\r\ndata_5[\"title_query_count\"] = data_5.groupby(\"title\")[\"query\"].transform(\"nunique\")","execution_count":null},{"metadata":{"id":"EC3ED40780504A9193E128AA66AC9B6D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"print(data_5[\"query_title_count\"].mean())\nprint(data_5[\"title_query_count\"].mean())\ndel data_5[\"query\"],data_5[\"title\"]\ngc.collect()","execution_count":null},{"metadata":{"id":"C76A16EB71F248E599B4F97B6AEB5B0A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 读取特征目录中的所有特征文件拼接到原数据上\nfeature_path=\"/home/kesci/work/features/train\"\nsl=os.listdir(feature_path)\nsl.remove('ctr_5.csv')\n\nfor i in sl:\n    temp_data=pd.read_csv(os.path.join(feature_path,i))\n    print(len(temp_data),i)\n    data_5 = pd.concat([data_5,temp_data],axis=1)\ndel temp_data\ngc.collect()\n\n\n# query和title长度的差值\ndata_5[\"dif_len\"] = data_5[\"query_len\"] - data_5[\"title_len\"] \n# query和title长度的比值\ndata_5[\"ql_dev_tl\"] = data_5[\"query_len\"] / data_5[\"title_len\"] \n# query组内的title长度均值\ndata_5[\"query_title_len_mean\"] = data_5.groupby(\"query_id\")[\"title_len\"].transform(\"mean\") \n# query的长度与query组内的title长度均值差值\ndata_5[\"q_dif_len_mean\"] = data_5[\"query_len\"] - data_5[\"query_title_len_mean\"]\n# title的长度与query组内的title长度均值差值\ndata_5[\"t_dif_len_mean\"] = data_5[\"title_len\"] - data_5[\"query_title_len_mean\"]\n# query的长度与query组内的title长度均值比值\ndata_5[\"q_dev_len_mean\"] = data_5[\"query_len\"] / data_5[\"query_title_len_mean\"]\n# title的长度与query组内的title长度均值比值\ndata_5[\"t_dev_len_mean\"] = data_5[\"title_len\"] / data_5[\"query_title_len_mean\"]","execution_count":null},{"metadata":{"id":"55BF8A87653E4FC08DFB84FBFD127526","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# sort_features要在query组内进行排序的特征\nsort_features=[\"BM25\",\"title_query_count\",\"cos_tfidf_distance\",\"sim_1\",\"t_dev_len_mean\",\"dis_1\",\n\"t_loc\",\"dis_8\",\"t_dif_len_mean\",\"loc_set_q\",\"long_same_lie_t\",\n\"loc_set_t\",\"set_rate\",\"long_same_total_loc\",\"dis_7\",\"long_same_density\",\"q_loc\",\n\"dis_2\",\"same_word_t\",\"sim_2\",\"dis_9\",\"sim_8\",\"dis_5\",\"sim_9\",\"dis_6\",\n\"title_len\",\"dis_4\",\n\"same_len_rate\",\n\"sim_6\",\"sim_7\",\"tri_gram\",\"sim_3\",\"sim_5\",\"jaro_winkler\",\"long_same_rate_t_len\",\n\"long_same_local_mean\",\n\"dis_3\",\"long_same_local_begin\",\"two_gram\",\"sim_4\",\"ql_dev_tl\",\"long_same_lie_q\",\n\"long_same_rate_q_len\"]\n\nfor fea in sort_features:\n    print(fea)\n    # 对特征进行组内排序\n    data_5[fea+'_sort']=data_5[fea].groupby(data_5['query_id']).rank(ascending=0,method='dense')\n    \ndel data_5[\"query_id\"],data_5[\"title_id\"]\ngc.collect()\n\n# 为了节省内存，将int64转化为int32\nfor i in data_5.columns:\n    if \"_sort\" in i:\n        print(i)\n        data_5[i]=data_5[i].astype(\"int32\")\n\n# 为了节省内存，将float64转化为float32\nfor i in data_5.dtypes.index:\n    if data_5.dtypes[i]==\"float64\":\n        print(i)\n        data_5[i]=data_5[i].astype(\"float32\")\n        \n        \ntrain_colums=list(data_5.columns)\ntrain_colums.remove(\"label\")\ntrain_x=data_5[train_colums[0]].values\ntrain_x=train_x.reshape([len(train_x),1])\nfor lie in train_colums[1:]:\n    temp_data=data_5[lie].values\n    temp_data=temp_data.reshape([len(temp_data),1])\n    train_x=np.c_[train_x,temp_data]\ndel temp_data\ngc.collect()\n\ntrain_y=data_5[\"label\"].values\nimport gc\ndel data_5\ngc.collect()\nX_train = train_x[:-50000]\ny_train = train_y[:-50000]\nX_test = train_x[-50000:]\ny_test = train_y[-50000:]\nimport gc\ndel train_x\ndel train_y\ngc.collect()\nlgb_train = lgb.Dataset(X_train,y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': {\"auc\",'binary_logloss'}, \n    'num_leaves': 76,\n    'max_depth': -1,\n     'subsample_freq':1,\n     'feature_fraction':0.8,\n    'learning_rate': 0.03,\n    'colsample_bytree':0.8,\n    'subsample':0.8,\n     'reg_alpha':0.0,\n     'reg_lambda':1,\n    'min_child_weight':50\n}\nprint('Start training...')\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=5000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=100)\ngbm.save_model('./model_file/gbm_good_model10.txt')","execution_count":null},{"metadata":{"id":"6DBC57983D60429EAE3650EFCE1F8FA7","mdEditEnable":false},"cell_type":"markdown","source":"# 生成第一个lgb模型的最终测试集的预测结果"},{"metadata":{"id":"93A83E7142BA44F98CCB3BD8696DF02F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"data_test = pd.read_csv(\"/home/kesci/test2.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\"])\nfeature_path=\"/home/kesci/work/features/test2\"\nsl=os.listdir(feature_path)\nfor i in sl:\n    temp_data=pd.read_csv(os.path.join(feature_path,i))\n    print(len(temp_data),i)\n    for i in temp_data.dtypes.index:\n        if temp_data.dtypes[i]==\"float64\":\n            print(i)\n            temp_data[i]=temp_data[i].astype(\"float32\")\n    data_test = pd.concat([data_test,temp_data],axis=1)\n    \n\ndata_test[\"dif_len\"] = data_test[\"query_len\"] - data_test[\"title_len\"]\ndata_test[\"ql_dev_tl\"] = data_test[\"query_len\"] / data_test[\"title_len\"]\ndata_test[\"query_title_len_mean\"] = data_test.groupby(\"query_id\")[\"title_len\"].transform(\"mean\")\ndata_test[\"q_dif_len_mean\"] = data_test[\"query_len\"] - data_test[\"query_title_len_mean\"]\ndata_test[\"t_dif_len_mean\"] = data_test[\"title_len\"] - data_test[\"query_title_len_mean\"]\ndata_test[\"q_dev_len_mean\"] = data_test[\"query_len\"] / data_test[\"query_title_len_mean\"]\ndata_test[\"t_dev_len_mean\"] = data_test[\"title_len\"] / data_test[\"query_title_len_mean\"]\ndata_test[\"query_title_count\"] = data_test.groupby(\"query\")[\"title\"].transform(\"nunique\")\ndata_test[\"title_query_count\"] = data_test.groupby(\"title\")[\"query\"].transform(\"nunique\")","execution_count":null},{"metadata":{"id":"6AA5E3DE233E410992A1FC40259E79E6","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"for fea in sort_features:\n    print(fea)\n    data_test[fea+'_sort']=data_test[fea].groupby(data_test['query_id']).rank(ascending=0,method='dense')\n    \nfor i in data_test.columns:\n    if \"_sort\" in i:\n        print(i)\n        data_test[i]=data_test[i].astype(\"int32\")\n\nfor i in data_test.dtypes.index:\n    if data_test.dtypes[i]==\"float64\":\n        print(i)\n        data_test[i]=data_test[i].astype(\"float32\")\n\ngbm=lgb.Booster(model_file='./model_file/gbm_good_model10.txt')\n\n\n#为节省内存，每500万转换成一次numpy矩阵\npreds=np.array([])\nfor i in range(0,len(data_test),5000000):\n    print(i)\n    temp_x=data_test.loc[i:i+4999999][train_colums].values\n    print(len(temp_x))\n    temp_preds = gbm.predict(temp_x, num_iteration=gbm.best_iteration)  \n    preds=np.r_[preds,temp_preds]\n\ntest_pred=pd.DataFrame(preds)\ntest_pred.columns=[\"prediction\"]\ntest_pred[\"query_id\"]=list(data_test[\"query_id\"])\ntest_pred[\"title_id\"]=list(data_test[\"title_id\"])\ntest_pred[[\"query_id\",\"title_id\",\"prediction\"]].to_csv('./submission/test2_lgb_baseline10.csv',index=None,header=0)","execution_count":null},{"metadata":{"id":"5E701A49AB404CD2BCE426612CE23229","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"# 将ctr特征全部分出来，单独训练一个lgb模型"},{"metadata":{"id":"FEC5634C178F49B789D78C043423A528"},"cell_type":"code","outputs":[],"source":"import lightgbm as lgb\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\nimport gc\r\nfrom sklearn.model_selection import train_test_split","execution_count":null},{"metadata":{"id":"B49F253508A742CFA29FF954E5144D6B"},"cell_type":"code","outputs":[],"source":"data_5 = pd.read_csv(\"/home/kesci/order_data/19.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\",\"label\"])","execution_count":null},{"metadata":{"id":"CDD874A84BEB467BA4EB63949177D33A"},"cell_type":"code","outputs":[],"source":"# 只加载ctr相关的特征\ntemp_data=pd.read_csv(\"/home/kesci/work/features/train/ctr_5.csv\")\ndata_5 = pd.concat([data_5,temp_data],axis=1)\ntemp_data=pd.read_csv(\"/home/kesci/train/query_title_word_ctr.csv\")\ndata_5 = pd.concat([data_5,temp_data],axis=1)","execution_count":null},{"metadata":{"id":"92D239E074B147E8A55441FBE0088935"},"cell_type":"code","outputs":[],"source":"import pickle\nall_title_C=pickle.load(open(\"/home/kesci/work/data/all_title_C.pkl\",\"rb\"))\ndef get_C(x):\n    if x in all_title_C:\n        return all_title_C[x]\n    else:\n        return 0\n        \ndef get_I(x):\n    if x in all_title_I:\n        return all_title_I[x]\n    else:\n        return 0\n\n# 得到每个title的展示次数\ndata_5[\"title_C\"]=data_5[\"title\"].apply(get_C)\ndel all_title_C\ngc.collect()\n\nall_title_I=pickle.load(open(\"/home/kesci/work/data/all_title_I.pkl\",\"rb\"))\n# 得到每个title的点击次数\ndata_5[\"title_I\"]=data_5[\"title\"].apply(get_I)\ndel all_title_I\ngc.collect()","execution_count":null},{"metadata":{"id":"AA64F61D847946A28A382210F736A284"},"cell_type":"code","outputs":[],"source":"# 使用上面计算出来的平滑因子，得到title的ctr\ndata_5[\"title_ctr\"]=(data_5[\"title_C\"]+1.3912001443482402)/(data_5[\"title_I\"]+1.3912001443482402+7.3260734359558795)","execution_count":null},{"metadata":{"id":"38DEBAB825AF428482AF15F4C9C99A91"},"cell_type":"code","outputs":[],"source":"train_colums=['5_ctr',\n 'best_query_word_ctr',\n 'best_title_word_ctr',\n 'sum_query_word_ctr',\n 'sum_title_word_ctr',\n 'title_C',\n 'title_I',\n 'title_ctr']","execution_count":null},{"metadata":{"id":"F9B70D49FBBA42CF8ACEB0C01C721225"},"cell_type":"code","outputs":[],"source":"params = {\r\n    'boosting_type': 'gbdt',\r\n    'objective': 'binary',\r\n    'metric': {\"auc\",'binary_logloss'},  #二进制对数损失\r\n    'num_leaves': 80,\r\n    'max_depth': 6,\r\n     'subsample_freq':1,\r\n     'feature_fraction':0.8,\r\n    'learning_rate': 0.03,\r\n     'reg_alpha':0.0,\r\n     'reg_lambda':1,\r\n}\r\n\r\nX, val_X, y, val_y = train_test_split(\r\n    train_x,\r\n    train_y,\r\n    test_size=0.001,\r\n    random_state=1,\r\n    stratify=train_y # 这里保证分割后y的比例分布与原数据一致\r\n)\r\nimport gc\r\ndel train_x\r\ndel train_y\r\ngc.collect()\r\nX_train = X\r\ny_train = y\r\nX_test = val_X\r\ny_test = val_y\r\nlgb_train = lgb.Dataset(X_train,y_train)\r\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)","execution_count":null},{"metadata":{"id":"04323CD81C95462E86250EB31CDAB990"},"cell_type":"code","outputs":[],"source":"gbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=5000,\r\n                valid_sets=lgb_eval,\r\n                early_stopping_rounds=100)\r\ngbm.save_model('./model_file/ctr_lgb_mode2.txt')","execution_count":null},{"metadata":{"id":"2569CDC0FDDA474F821FE29D739A26B9","mdEditEnable":false},"cell_type":"markdown","source":"# 得到最终测试集在ctr_lgb上的预测结果"},{"metadata":{"id":"1D536D53CA724BDF8446D5CFB9683FBF"},"cell_type":"code","outputs":[],"source":"title_I_19=pickle.load(open(\"/home/kesci/title_I_19.pkl\",\"rb\"))\ntitle_C_19=pickle.load(open(\"/home/kesci/title_C_19.pkl\",\"rb\"))","execution_count":null},{"metadata":{"id":"4AA4DA2A53D24702B0B0E7DBD7CF6F9D"},"cell_type":"code","outputs":[],"source":"def get_ctr_5(x):\n    if x not in title_I_19:\n        return 1.3912001443482402/(1.3912001443482402+7.3260734359558795)\n    else:\n        return (title_C_19[x]+1.3912001443482402)/(title_I_19[x]+1.3912001443482402+7.3260734359558795)","execution_count":null},{"metadata":{"id":"FB36F29992704A599C17416870ACBF38"},"cell_type":"code","outputs":[],"source":"data_test = pd.read_csv(\"/home/kesci/test2.csv\",names=[\"query_id\",\"query\",\"title_id\",\"title\"])\ndata_test[\"5_ctr\"]=data_test[\"title\"].apply(get_ctr_5)\nall_title_C=pickle.load(open(\"/home/kesci/work/data/all_title_C.pkl\",\"rb\"))\n\ndef get_C(x):\n    if x in all_title_C:\n        return all_title_C[x]\n    else:\n        return 0\n        \ndef get_I(x):\n    if x in all_title_I:\n        return all_title_I[x]\n    else:\n        return 0\n\ndata_test[\"title_C\"]=data_test[\"title\"].apply(get_C)\ndel all_title_C\ngc.collect()\n\nall_title_I=pickle.load(open(\"/home/kesci/work/data/all_title_I.pkl\",\"rb\"))\ndata_test[\"title_I\"]=data_test[\"title\"].apply(get_I)\ndel all_title_I\ngc.collect()\n\ndata_test[\"title_ctr\"]=(data_test[\"title_C\"]+1.3912001443482402)/(data_test[\"title_I\"]+1.3912001443482402+7.3260734359558795)","execution_count":null},{"metadata":{"id":"964E16131AFE481C87C5C63719DFF632"},"cell_type":"code","outputs":[],"source":"import pandas as pd\ntemp_data = pd.read_csv(\"/home/kesci/test2/query_title_word_ctr.csv\")\ndata_test = pd.concat([data_test,temp_data],axis=1)","execution_count":null},{"metadata":{"id":"95A8105B12C84569AE542171C0E1868D"},"cell_type":"code","outputs":[],"source":"test_x=data_test[train_colums].values","execution_count":null},{"metadata":{"id":"0C32FB05C1A345B58BF17157C7BC1BE3"},"cell_type":"code","outputs":[],"source":"gbm=lgb.Booster(model_file='./model_file/ctr_lgb_mode2.txt')\r\npreds = gbm.predict(test_x, num_iteration=gbm.best_iteration)  \r\n\r\n\r\ntest_pred=pd.DataFrame(preds)\r\ntest_pred.columns=[\"prediction\"]\r\ntest_pred[\"query_id\"]=list(data_test[\"query_id\"])\r\ntest_pred[\"title_id\"]=list(data_test[\"title_id\"])\r\ntest_pred[[\"query_id\",\"title_id\",\"prediction\"]].to_csv('./submission/test2_ctr_lgb_pre2.csv',index=None,header=0)","execution_count":null},{"metadata":{"id":"98388F0873A241718F29554663DAEE12","mdEditEnable":false},"cell_type":"markdown","source":"# 神经网络模型ESIM，双端纯文本输入，未加特征"},{"metadata":{"id":"8A11DB12E76744E9BC5DBF0FCA8781B8"},"cell_type":"code","outputs":[],"source":"# 加载并处理数据，对输入的文本进行编码处理，词转化成id，并对长度进行了对齐。\r\ndef load_esim_data_and_labels(data_path,char_to_id,q_max_len=22,t_max_len=22):\r\n    f = open(data_path, 'r')\r\n    y = []\r\n    t_feat_index = []\r\n    q_feat_index = []\r\n    for line in f:\r\n        line = line.strip().split(\",\")\r\n        y.append([int(line[4])])\r\n        query = []\r\n        for i in line[1].split():\r\n            if i in char_to_id:\r\n                query.append(char_to_id[i])\r\n            else:\r\n                query.append(0)\r\n        if len(query) < q_max_len:\r\n            query = query + [0] * (q_max_len - len(query))\r\n        else:\r\n            query = query[:q_max_len]\r\n        q_feat_index.append(query)\r\n        title = []\r\n        for i in line[3].split():\r\n            if i in char_to_id:\r\n                title.append(char_to_id[i])\r\n            else:\r\n                title.append(0)\r\n        if len(title) < t_max_len:\r\n            title = title + [0] * (t_max_len - len(title))\r\n        else:\r\n            title = title[:t_max_len]\r\n        t_feat_index.append(title)\r\n    f.close()\r\n    return {\"q_feat_index\": q_feat_index,\"t_feat_index\": t_feat_index,\"label\": y}\r\n\r\n# 一批一批加载并处理数据，对输入的文本进行编码处理，词转化成id，并对长度进行了对齐。\r\ndef yield_esim_data_and_labels(data_path,char_to_id,batch_size,q_max_len=22,t_max_len=22):\r\n    f = open(data_path, 'r')\r\n    y = []\r\n    t_feat_index = []\r\n    q_feat_index = []\r\n    temp_b=0\r\n    for line in f:\r\n        line = line.strip().split(\",\")\r\n        y.append([int(line[4])])\r\n        query = []\r\n        for i in line[1].split():\r\n            if i in char_to_id:\r\n                query.append(char_to_id[i])\r\n            else:\r\n                query.append(0)\r\n        if len(query) < q_max_len:\r\n            query = query + [0] * (q_max_len - len(query))\r\n        else:\r\n            query = query[:q_max_len]\r\n        q_feat_index.append(query)\r\n        title = []\r\n        for i in line[3].split():\r\n            if i in char_to_id:\r\n                title.append(char_to_id[i])\r\n            else:\r\n                title.append(0)\r\n        if len(title) < t_max_len:\r\n            title = title + [0] * (t_max_len - len(title))\r\n        else:\r\n            title = title[:t_max_len]\r\n        t_feat_index.append(title)\r\n        temp_b += 1\r\n        if temp_b == batch_size:\r\n            yield {\"q_feat_index\": q_feat_index, \"t_feat_index\": t_feat_index, \"label\": y}\r\n            y = []\r\n            t_feat_index = []\r\n            q_feat_index = []\r\n            temp_b = 0\r\n    if temp_b!=0:\r\n        yield {\"q_feat_index\": q_feat_index,\"t_feat_index\": t_feat_index,\"label\": y}\r\n    f.close()","execution_count":null},{"metadata":{"id":"7E2F5A87E7B8414E891F5B432F6ED29C"},"cell_type":"code","outputs":[],"source":"from torch import nn\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport os\r\nimport numpy as np\r\nfrom time import time\r\nfrom torch.autograd import Variable\r\nfrom gensim.models import Word2Vec\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom gensim.models import FastText\r\n\r\nword2id_file=\"./data/word2id.txt\"\r\nread_file = open(word2id_file, \"r\")\r\nword2id={}\r\nfor i in read_file:\r\n    i=i.strip().split()\r\n    word2id[i[0]]=int(i[1])\r\nread_file.close()\r\n\r\n#参数配置\r\nepcho=1\r\nbatch_size=256 \r\nnum_to_ev=400 # 训练多少批，在本地评测一次\r\nvocab_size=len(word2id) # 词典大小\r\nembedding_dim=256 # 词向量维度\r\nt_max_len=22 #title的最大长度\r\nq_max_len=11 #query的最大长度\r\nlr=0.0001 #学习率\r\n\r\n\r\n\r\n#加载验证集\r\nval_data=load_esim_data_and_labels(\"/home/kesci/work/data/eval_data/19_eval.csv\",word2id,q_max_len=q_max_len,t_max_len=t_max_len)\r\n\r\n\r\n# 每个词拼接使用128维的w2v和128维的fast向量到256维的组合向量表示\r\nce = np.random.uniform(-1, 1, [vocab_size + 1,embedding_dim])\r\nword2vec_model = Word2Vec.load(\"/home/kesci/word2vec.model\")\r\nfast_model = FastText.load(\"./data/fast_w2v.model\")\r\nce[0] = np.zeros(embedding_dim)\r\nfor i in word2id:\r\n    try:\r\n        ce[word2id[i]] = np.concatenate((word2vec_model[i],fast_model[i]))\r\n    except:\r\n        print(i)","execution_count":null},{"metadata":{"id":"84159983CDC840CD827F5C277722FFCD"},"cell_type":"code","outputs":[],"source":"# 注意力机制padding部分，赋予极小的数，这样算出来的padding部分注意力分数无限接近0，减少噪音。\r\nmask_num=-2**32+1.0\r\n# 模型结构，标准的esim结构\r\nclass ESIM(nn.Module):\r\n    def __init__(self,hidden_size,embeds_dim,linear_size):\r\n        super(ESIM, self).__init__()\r\n        self.verbose=True\r\n        self.dropout = 0.5\r\n        self.n_epochs=epcho\r\n        self.learning_rate=lr\r\n        self.optimizer_type=\"adam\"\r\n        self.use_cuda=True\r\n        self.batch_size=batch_size\r\n        self.eval_metric=roc_auc_score\r\n        self.hidden_size = hidden_size\r\n        self.embeds_dim = embeds_dim\r\n        num_word = vocab_size\r\n        self.embeds = nn.Embedding(num_word+1, self.embeds_dim,padding_idx=0)\r\n        self.embeds.weight.data.copy_(torch.from_numpy(ce)) # 使用ce矩阵，初始化embedding矩阵。\r\n\r\n        self.bn_embeds = nn.BatchNorm1d(self.embeds_dim)\r\n        self.lstm1 = nn.LSTM(self.embeds_dim, self.hidden_size, batch_first=True, bidirectional=True)\r\n        self.lstm2 = nn.LSTM(self.hidden_size*8, self.hidden_size, batch_first=True, bidirectional=True)\r\n\r\n        self.fc = nn.Sequential(\r\n            nn.BatchNorm1d(self.hidden_size * 8),\r\n            nn.Linear(self.hidden_size * 8, linear_size),\r\n            nn.ELU(inplace=True),\r\n            nn.BatchNorm1d(linear_size),\r\n            nn.Dropout(self.dropout),\r\n            nn.Linear(linear_size, linear_size),\r\n            nn.ELU(inplace=True),\r\n            nn.BatchNorm1d(linear_size),\r\n            nn.Dropout(self.dropout),\r\n            nn.Linear(linear_size, 1)\r\n        )\r\n    \r\n    def soft_attention_align(self, x1, x2, mask1, mask2):\r\n        '''\r\n        x1: batch_size * seq_len * dim\r\n        x2: batch_size * seq_len * dim\r\n        '''\r\n        # attention: batch_size * seq_len * seq_len\r\n        attention = torch.matmul(x1, x2.transpose(1, 2))\r\n        mask1 = mask1.float().masked_fill_(mask1, mask_num)\r\n        mask2 = mask2.float().masked_fill_(mask2, mask_num)\r\n\r\n        # weight: batch_size * seq_len * seq_len\r\n        weight1 = F.softmax(attention + mask2.unsqueeze(1), dim=-1)\r\n        x1_align = torch.matmul(weight1, x2)\r\n        weight2 = F.softmax(attention.transpose(1, 2) + mask1.unsqueeze(1), dim=-1)\r\n        x2_align = torch.matmul(weight2, x1)\r\n        # x_align: batch_size * seq_len * hidden_size\r\n\r\n        return x1_align, x2_align\r\n\r\n    def submul(self, x1, x2):\r\n        mul = x1 * x2\r\n        sub = x1 - x2\r\n        return torch.cat([sub, mul], -1)\r\n\r\n    def apply_multiple(self, x):\r\n        # input: batch_size * seq_len * (2 * hidden_size)\r\n        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\r\n        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\r\n        # output: batch_size * (4 * hidden_size)\r\n        return torch.cat([p1, p2], 1)\r\n\r\n    def forward(self, *input):\r\n        # batch_size * seq_len\r\n        sent1, sent2 = input[0], input[1]\r\n        mask1, mask2 = sent1.eq(0), sent2.eq(0)\r\n\r\n        # embeds: batch_size * seq_len => batch_size * seq_len * dim\r\n        x1 = self.bn_embeds(self.embeds(sent1).transpose(1, 2).contiguous()).transpose(1, 2)\r\n        x2 = self.bn_embeds(self.embeds(sent2).transpose(1, 2).contiguous()).transpose(1, 2)\r\n\r\n        # batch_size * seq_len * dim => batch_size * seq_len * hidden_size\r\n        o1, _ = self.lstm1(x1)\r\n        o2, _ = self.lstm1(x2)\r\n\r\n        # Attention\r\n        # batch_size * seq_len * hidden_size\r\n        q1_align, q2_align = self.soft_attention_align(o1, o2, mask1, mask2)\r\n        \r\n        # Compose\r\n        # batch_size * seq_len * (8 * hidden_size)\r\n        q1_combined = torch.cat([o1, q1_align, self.submul(o1, q1_align)], -1)\r\n        q2_combined = torch.cat([o2, q2_align, self.submul(o2, q2_align)], -1)\r\n\r\n        # batch_size * seq_len * (2 * hidden_size)\r\n        q1_compose, _ = self.lstm2(q1_combined)\r\n        q2_compose, _ = self.lstm2(q2_combined)\r\n\r\n        # Aggregate\r\n        # input: batch_size * seq_len * (2 * hidden_size)\r\n        # output: batch_size * (4 * hidden_size)\r\n        q1_rep = self.apply_multiple(q1_compose)\r\n        q2_rep = self.apply_multiple(q2_compose)\r\n\r\n        # Classifier\r\n        x = torch.cat([q1_rep, q2_rep], -1)\r\n        similarity = self.fc(x)\r\n        return similarity\r\n\r\n    def fit(self,save_path = None):\r\n        if save_path and not os.path.exists('/'.join(save_path.split('/')[0:-1])):\r\n            print(\"Save path is not existed!\")\r\n            return\r\n\r\n        if self.verbose:\r\n            print(\"pre_process data ing...\")\r\n        if self.verbose:\r\n            print(\"pre_process data finished\")\r\n\r\n        model = self.train()\r\n\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\r\n        if self.optimizer_type == 'adam':\r\n            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n        elif self.optimizer_type == 'rmsp':\r\n            optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\r\n        elif self.optimizer_type == 'adag':\r\n            optimizer = torch.optim.Adagrad(self.parameters(), lr=self.learning_rate)\r\n\r\n        criterion = F.binary_cross_entropy_with_logits\r\n        max_eval=0.60\r\n        valid_result = []\r\n        for epoch in range(self.n_epochs):\r\n            total_loss = 0.0\r\n            for train_dict in yield_esim_data_and_labels(\"/home/kesci/shuffle_data.csv\", word2id, num_to_ev*batch_size,\r\n                                                        q_max_len=q_max_len,t_max_len=t_max_len):\r\n                query=train_dict['q_feat_index']\r\n                title=train_dict['t_feat_index']\r\n                y_train=train_dict['label']\r\n                query = np.array(query)\r\n                y_train = np.array(y_train, dtype=np.float32)\r\n                x_size = query.shape[0]\r\n                batch_iter = x_size // self.batch_size\r\n                epoch_begin_time = time()\r\n                batch_begin_time = time()\r\n                for i in range(batch_iter+1):\r\n                    offset = i*self.batch_size\r\n                    end = min(x_size, offset+self.batch_size)\r\n                    if offset == end:\r\n                        break\r\n                    batch_query = Variable(torch.LongTensor(query[offset:end]))\r\n                    batch_title = Variable(torch.LongTensor(title[offset:end]))\r\n\r\n                    batch_y = Variable(torch.FloatTensor(y_train[offset:end]))\r\n                    if self.use_cuda:\r\n                        batch_query,batch_title,batch_y = batch_query.cuda(), batch_title.cuda(),batch_y.cuda()\r\n                    optimizer.zero_grad()\r\n                    #print(batch_query,batch_title)\r\n                    outputs = model(batch_query,batch_title)\r\n                    #print(outputs)\r\n                    loss = criterion(outputs, batch_y)\r\n                    loss.backward()\r\n                    optimizer.step()\r\n                    for p in model.parameters():\r\n                        if p.grad is not None:\r\n                            del p.grad  # free some memory\r\n                    torch.cuda.empty_cache()\r\n                    #model = self.train()\r\n\r\n                    total_loss += loss.item()\r\n                    if self.verbose:\r\n                        if i % 100 == 99:\r\n                            eval = self.evaluate(batch_query,batch_title,batch_y)\r\n                            print('[%d, %5d] loss: %.6f metric: %.6f time: %.1f s' %\r\n                                  (epoch + 1, i + 1, total_loss/100.0, eval, time()-batch_begin_time))\r\n                            total_loss = 0.0\r\n                            batch_begin_time = time()\r\n                            for p in model.parameters():\r\n                                if p.grad is not None:\r\n                                    del p.grad  # free some memory\r\n                            torch.cuda.empty_cache()\r\n                            model = self.train()\r\n\r\n                query_valid = val_data[\"q_feat_index\"]\r\n                title_valid = val_data[\"t_feat_index\"]\r\n                y_valid = val_data[\"label\"]\r\n                query_valid = np.array(query_valid)\r\n                title_valid = np.array(title_valid)\r\n                y_valid = np.array(y_valid,dtype=np.float32)\r\n                x_valid_size = query_valid.shape[0]\r\n                valid_loss, valid_eval = self.eval_by_batch(query_valid,title_valid,y_valid, x_valid_size)\r\n                valid_result.append(valid_eval)\r\n                print('*' * 50)\r\n                print('[%d] loss: %.6f val_metric: %.6f time: %.1f s' %\r\n                      (epoch + 1, valid_loss, valid_eval,time()-epoch_begin_time))\r\n                print('*' * 50)\r\n                if save_path and valid_eval>max_eval:\r\n                    max_eval=valid_eval\r\n                    torch.save(self, save_path)\r\n                    print(\"保存模型成功\")\r\n                for p in model.parameters():\r\n                    if p.grad is not None:\r\n                        del p.grad  # free some memory\r\n                torch.cuda.empty_cache()\r\n                model = self.train()\r\n                #torch.save(self.state_dict(),save_path)\r\n\r\n    def eval_by_batch(self,query,title,y, x_size):\r\n        total_loss = 0.0\r\n        y_pred = []\r\n        batch_size = self.batch_size\r\n        batch_iter = x_size // batch_size\r\n        criterion = F.binary_cross_entropy_with_logits\r\n        model = self.eval()\r\n        for i in range(batch_iter+1):\r\n            offset = i * batch_size\r\n            end = min(x_size, offset + batch_size)\r\n            if offset == end:\r\n                break\r\n            batch_query = Variable(torch.LongTensor(query[offset:end]))\r\n            batch_title = Variable(torch.LongTensor(title[offset:end]))\r\n            batch_y = Variable(torch.FloatTensor(y[offset:end]))\r\n            if self.use_cuda:\r\n                batch_query,batch_title,batch_y = batch_query.cuda(),batch_title.cuda(),batch_y.cuda()\r\n            \r\n            with torch.no_grad():\r\n                outputs = model(batch_query,batch_title)\r\n                pred = torch.sigmoid(outputs).cpu()\r\n                y_pred.extend(pred.data.numpy())\r\n                loss = criterion(outputs, batch_y)\r\n                total_loss += loss.item()*(end-offset)\r\n        total_metric = self.eval_metric(y,y_pred)\r\n        return total_loss/x_size, total_metric\r\n\r\n    def predict_proba(self, batch_q,batch_t):\r\n        batch_q = np.array(batch_q)\r\n        batch_t = np.array(batch_t)\r\n        batch_q = Variable(torch.LongTensor(batch_q))\r\n        batch_t = Variable(torch.LongTensor(batch_t))\r\n        if self.use_cuda and torch.cuda.is_available():\r\n            batch_q = batch_q.cuda()\r\n            batch_t = batch_t.cuda()\r\n\r\n        model = self.eval()\r\n        with torch.no_grad():\r\n            pred = torch.sigmoid(model(batch_q,batch_t)).cpu()\r\n        return pred.data.numpy()\r\n\r\n\r\n    def inner_predict_proba(self,batch_q,batch_t):\r\n        model = self.eval()\r\n        with torch.no_grad():\r\n            pred = torch.sigmoid(model(batch_q,batch_t)).cpu()\r\n        return pred.data.numpy()\r\n\r\n\r\n    def evaluate(self, batch_q,batch_t,y):\r\n        y_pred = self.inner_predict_proba(batch_q,batch_t)\r\n        return self.eval_metric(y.cpu().data.numpy(), y_pred)","execution_count":null},{"metadata":{"id":"B0F24FD935BC44F088196F89F9F0FD95"},"cell_type":"code","outputs":[],"source":"model = ESIM(hidden_size=256,embeds_dim=embedding_dim,linear_size=256).cuda()\nmodel.fit(save_path=\"./model_file/esim_model.pkl\")","execution_count":null},{"metadata":{"id":"6BDD17F3A10D4F72BA5BEE7C172F52C2","mdEditEnable":false},"cell_type":"markdown","source":"# 使用ESIM模型生成最终测试集的结果"},{"metadata":{"id":"DC872851380F4388AED4F877DD35CF73"},"cell_type":"code","outputs":[],"source":"#一批一批加载数据，到内存，对输入的文本进行编码处理，词转化成id，并对长度进行了对齐，并返回query_id和title_id\r\ndef yield_test_esim_data(data_path,char_to_id,batch_size,q_max_len=22,t_max_len=22):\r\n    f = open(data_path, 'r')\r\n    t_feat_index = []\r\n    q_feat_index = []\r\n    query_ids=[]\r\n    title_ids=[]\r\n    temp_b=0\r\n    for line in f:\r\n        line = line.strip().split(\",\")\r\n        query = []\r\n        for i in line[1].split():\r\n            if i in char_to_id:\r\n                query.append(char_to_id[i])\r\n            else:\r\n                query.append(0)\r\n        if len(query) < q_max_len:\r\n            query = query + [0] * (q_max_len - len(query))\r\n        else:\r\n            query = query[:q_max_len]\r\n        q_feat_index.append(query)\r\n        query_ids.append(line[0])\r\n        title_ids.append(line[2])\r\n        title = []\r\n        for i in line[3].split():\r\n            if i in char_to_id:\r\n                title.append(char_to_id[i])\r\n            else:\r\n                title.append(0)\r\n        if len(title) < t_max_len:\r\n            title = title + [0] * (t_max_len - len(title))\r\n        else:\r\n            title = title[:t_max_len]\r\n        t_feat_index.append(title)\r\n        temp_b += 1\r\n        if temp_b == batch_size:\r\n            yield {\"q_feat_index\": q_feat_index, \"t_feat_index\": t_feat_index,\"query_ids\":query_ids,\"title_ids\":title_ids}\r\n            t_feat_index = []\r\n            q_feat_index = []\r\n            query_ids=[]\r\n            title_ids=[]\r\n            temp_b = 0\r\n    if temp_b!=0:\r\n        yield {\"q_feat_index\": q_feat_index, \"t_feat_index\": t_feat_index,\"query_ids\":query_ids,\"title_ids\":title_ids}\r\n    f.close()","execution_count":null},{"metadata":{"id":"A8D3D4ACF125455BA68EB8A238A97923"},"cell_type":"code","outputs":[],"source":"from torch import nn\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport os\r\nimport numpy as np\r\nfrom time import time\r\nfrom torch.autograd import Variable\r\nfrom sklearn.metrics import roc_auc_score\r\nimport pandas as pd\r\n\r\n\r\n\r\n\r\nword2id_file=\"./data/word2id.txt\"\r\nread_file = open(word2id_file, \"r\")\r\nword2id={}\r\nfor i in read_file:\r\n    i=i.strip().split()\r\n    word2id[i[0]]=int(i[1])\r\nread_file.close()\r\n\r\nnum_to_ev=100\r\nbatch_size=2048\r\nvocab_size=len(word2id)\r\nembedding_dim=256\r\nt_max_len=22\r\nq_max_len=11\r\nlr=0.0003","execution_count":null},{"metadata":{"id":"53CE37CDB6AF4A648D4DC8F5AFB007F7"},"cell_type":"code","outputs":[],"source":"mask_num=-2**32+1.0\r\nclass ESIM(nn.Module):\r\n    def __init__(self,hidden_size,embeds_dim,linear_size):\r\n        super(ESIM, self).__init__()\r\n        self.verbose=True\r\n        self.dropout = 0.5\r\n        self.n_epochs=epcho\r\n        self.learning_rate=lr\r\n        self.optimizer_type=\"adam\"\r\n        self.use_cuda=False\r\n        self.batch_size=batch_size\r\n        self.eval_metric=roc_auc_score\r\n        self.hidden_size = hidden_size\r\n        self.embeds_dim = embeds_dim\r\n        num_word = vocab_size\r\n        self.embeds = nn.Embedding(num_word+1, self.embeds_dim,padding_idx=0)\r\n\r\n        self.bn_embeds = nn.BatchNorm1d(self.embeds_dim)\r\n        self.lstm1 = nn.LSTM(self.embeds_dim, self.hidden_size, batch_first=True, bidirectional=True)\r\n        self.lstm2 = nn.LSTM(self.hidden_size*8, self.hidden_size, batch_first=True, bidirectional=True)\r\n\r\n        self.fc = nn.Sequential(\r\n            nn.BatchNorm1d(self.hidden_size * 8),\r\n            nn.Linear(self.hidden_size * 8, linear_size),\r\n            nn.ELU(inplace=True),\r\n            nn.BatchNorm1d(linear_size),\r\n            nn.Dropout(self.dropout),\r\n            nn.Linear(linear_size, linear_size),\r\n            nn.ELU(inplace=True),\r\n            nn.BatchNorm1d(linear_size),\r\n            nn.Dropout(self.dropout),\r\n            nn.Linear(linear_size, 1)\r\n        )\r\n    \r\n    def soft_attention_align(self, x1, x2, mask1, mask2):\r\n        '''\r\n        x1: batch_size * seq_len * dim\r\n        x2: batch_size * seq_len * dim\r\n        '''\r\n        # attention: batch_size * seq_len * seq_len\r\n        attention = torch.matmul(x1, x2.transpose(1, 2))\r\n        mask1 = mask1.float().masked_fill_(mask1, mask_num)\r\n        mask2 = mask2.float().masked_fill_(mask2, mask_num)\r\n\r\n        # weight: batch_size * seq_len * seq_len\r\n        weight1 = F.softmax(attention + mask2.unsqueeze(1), dim=-1)\r\n        x1_align = torch.matmul(weight1, x2)\r\n        weight2 = F.softmax(attention.transpose(1, 2) + mask1.unsqueeze(1), dim=-1)\r\n        x2_align = torch.matmul(weight2, x1)\r\n        # x_align: batch_size * seq_len * hidden_size\r\n\r\n        return x1_align, x2_align\r\n\r\n    def submul(self, x1, x2):\r\n        mul = x1 * x2\r\n        sub = x1 - x2\r\n        return torch.cat([sub, mul], -1)\r\n\r\n    def apply_multiple(self, x):\r\n        # input: batch_size * seq_len * (2 * hidden_size)\r\n        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\r\n        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\r\n        # output: batch_size * (4 * hidden_size)\r\n        return torch.cat([p1, p2], 1)\r\n\r\n    def forward(self, *input):\r\n        # batch_size * seq_len\r\n        sent1, sent2 = input[0], input[1]\r\n        mask1, mask2 = sent1.eq(0), sent2.eq(0)\r\n\r\n        # embeds: batch_size * seq_len => batch_size * seq_len * dim\r\n        x1 = self.bn_embeds(self.embeds(sent1).transpose(1, 2).contiguous()).transpose(1, 2)\r\n        x2 = self.bn_embeds(self.embeds(sent2).transpose(1, 2).contiguous()).transpose(1, 2)\r\n\r\n        # batch_size * seq_len * dim => batch_size * seq_len * hidden_size\r\n        o1, _ = self.lstm1(x1)\r\n        o2, _ = self.lstm1(x2)\r\n\r\n        # Attention\r\n        # batch_size * seq_len * hidden_size\r\n        q1_align, q2_align = self.soft_attention_align(o1, o2, mask1, mask2)\r\n        \r\n        # Compose\r\n        # batch_size * seq_len * (8 * hidden_size)\r\n        q1_combined = torch.cat([o1, q1_align, self.submul(o1, q1_align)], -1)\r\n        q2_combined = torch.cat([o2, q2_align, self.submul(o2, q2_align)], -1)\r\n\r\n        # batch_size * seq_len * (2 * hidden_size)\r\n        q1_compose, _ = self.lstm2(q1_combined)\r\n        q2_compose, _ = self.lstm2(q2_combined)\r\n\r\n        # Aggregate\r\n        # input: batch_size * seq_len * (2 * hidden_size)\r\n        # output: batch_size * (4 * hidden_size)\r\n        q1_rep = self.apply_multiple(q1_compose)\r\n        q2_rep = self.apply_multiple(q2_compose)\r\n\r\n        # Classifier\r\n        x = torch.cat([q1_rep, q2_rep], -1)\r\n        similarity = self.fc(x)\r\n        return similarity\r\n\r\n    def eval_by_batch(self,query,title,y, x_size):\r\n        total_loss = 0.0\r\n        y_pred = []\r\n        batch_size = self.batch_size\r\n        batch_iter = x_size // batch_size\r\n        criterion = F.binary_cross_entropy_with_logits\r\n        model = self.eval()\r\n        for i in range(batch_iter+1):\r\n            offset = i * batch_size\r\n            end = min(x_size, offset + batch_size)\r\n            if offset == end:\r\n                break\r\n            batch_query = Variable(torch.LongTensor(query[offset:end]))\r\n            batch_title = Variable(torch.LongTensor(title[offset:end]))\r\n            batch_y = Variable(torch.FloatTensor(y[offset:end]))\r\n            if self.use_cuda:\r\n                batch_query,batch_title,batch_y = batch_query.cuda(),batch_title.cuda(),batch_y.cuda()\r\n            \r\n            with torch.no_grad():\r\n                outputs = model(batch_query,batch_title)\r\n                pred = torch.sigmoid(outputs).cpu()\r\n                y_pred.extend(pred.data.numpy())\r\n                loss = criterion(outputs, batch_y)\r\n                total_loss += loss.item()*(end-offset)\r\n        total_metric = self.eval_metric(y,y_pred)\r\n        return total_loss/x_size, total_metric\r\n\r\n    def predict_proba(self, batch_q,batch_t):\r\n        batch_q = np.array(batch_q)\r\n        batch_t = np.array(batch_t)\r\n        batch_q = Variable(torch.LongTensor(batch_q))\r\n        batch_t = Variable(torch.LongTensor(batch_t))\r\n        if self.use_cuda and torch.cuda.is_available():\r\n            batch_q = batch_q.cuda()\r\n            batch_t = batch_t.cuda()\r\n\r\n        model = self.eval()\r\n        with torch.no_grad():\r\n            pred = torch.sigmoid(model(batch_q,batch_t)).cpu()\r\n        return pred.data.numpy()\r\n\r\n\r\n    def inner_predict_proba(self,batch_q,batch_t):\r\n        \"\"\"\r\n        :param Xi: tensor of feature index\r\n        :param Xv: tensor of feature value\r\n        :return: output, numpy\r\n        \"\"\"\r\n        model = self.eval()\r\n        with torch.no_grad():\r\n            pred = torch.sigmoid(model(batch_q,batch_t)).cpu()\r\n        return pred.data.numpy()\r\n\r\n\r\n    def evaluate(self, batch_q,batch_t,y):\r\n        \"\"\"\r\n        :param Xi: tensor of feature index\r\n        :param Xv: tensor of feature value\r\n        :param y: tensor of labels\r\n        :return: metric of the evaluation\r\n        \"\"\"\r\n        y_pred = self.inner_predict_proba(batch_q,batch_t)\r\n        return self.eval_metric(y.cpu().data.numpy(), y_pred)","execution_count":null},{"metadata":{"id":"5B0C6664CF2949FAB7C6673F78DBD048"},"cell_type":"code","outputs":[],"source":"esim_model = torch.load('/home/kesci/work/model_file/esim_model.pkl').cuda()","execution_count":null},{"metadata":{"id":"D584B27DBD2E4D4D887BEECECF7F4DD7"},"cell_type":"code","outputs":[],"source":"outputfile=open(\"./submission/test2_esim_pre.csv\",\"w\")\r\nall_ress=[]\r\nall_len=0\r\nfor test_dict in yield_test_esim_data(\"/home/kesci/test2.csv\", word2id, num_to_ev*batch_size,\r\n                                                        q_max_len=q_max_len,t_max_len=t_max_len):\r\n    ress=[]\r\n    query = test_dict['q_feat_index']\r\n    title = test_dict['t_feat_index']\r\n    query = np.array(query)\r\n    x_size = query.shape[0]\r\n    batch_iter = x_size // batch_size\r\n\r\n    for i in range(batch_iter + 1):\r\n        offset = i * batch_size\r\n        end = min(x_size, offset + batch_size)\r\n        if offset == end:\r\n            break\r\n        batch_query = query[offset:end]\r\n        batch_title = title[offset:end]\r\n        res = esim_model.predict_proba(batch_query,batch_title)\r\n        ress.extend(list([mm[0] for mm in res]))\r\n        all_ress.extend(list([mm[0] for mm in res]))\r\n    all_len+=x_size\r\n    print(all_len)\r\n    for dui in zip(test_dict[\"query_ids\"],test_dict[\"title_ids\"],ress):\r\n        outputfile.write(str(dui[0])+\",\"+str(dui[1])+\",\"+str(dui[2])+\"\\n\")\r\noutputfile.close()","execution_count":null},{"metadata":{"id":"757ED21602244E5289F272EE7ABBF9E9","mdEditEnable":false},"cell_type":"markdown","source":"# 模型融合：简单将三个模型的结果文件，加和取平均即可。"},{"metadata":{"id":"DEFB432F281640FFA15DA95A8EFAFEFB"},"cell_type":"code","outputs":[],"source":"import pandas as pd\r\nfinal_ctr_lgb_pre = pd.read_csv(\"/home/kesci/work/submission/test2_ctr_lgb_pre2.csv\",\r\nnames=[\"q_id\",\"t_id\",\"pre\"])\r\nfinal_test2 = pd.read_csv('/home/kesci/work/submission/test2_esim_pre.csv',names=[\"q_id\",\"t_id\",\"pre\"])\r\nfinal_test3 = pd.read_csv('/home/kesci/work/submission/test2_lgb_baseline10.csv',names=[\"q_id\",\"t_id\",\"pre\"])","execution_count":null},{"metadata":{"id":"F099C89DB3BA4500B284C7220BDEC27F"},"cell_type":"code","outputs":[],"source":"final_test3[\"final_pre\"]=(final_ctr_lgb_pre[\"pre\"]+final_test2[\"pre\"]+final_test3[\"pre\"])/3","execution_count":null},{"metadata":{"id":"73841F5AE3B84EE78BEC3DF22C2706DC"},"cell_type":"code","outputs":[],"source":"# test2_sub1.csv 即为最后的结果文件\nfinal_test3[[\"q_id\",\"t_id\",\"final_pre\"]].to_csv(\"/home/kesci/work/final_sub/test2_sub1.csv\",\nindex=None,header=0)","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}